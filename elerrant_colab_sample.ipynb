{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "elerrant_to_git.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNNx9mWnBo0pXTmmDE3FMxa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/katkorre/elerrant/blob/main/elerrant.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_MVSoFBBr4n3"
      },
      "source": [
        "%%capture\n",
        "#running this will place everything we need in the directory. \n",
        "! git clone https://github.com/chrisjbryant/errant.git"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZIA3ZEcsl_u"
      },
      "source": [
        "%%capture\n",
        "#demo given that there is an orig.txt, cor.txt in the directory.\n",
        "# orig text \n",
        "! gdown --id 157UeUcZXrRMEIKr6B9JKWk9hbMVOMMEa\n",
        "# cor text \n",
        "! gdown --id 1Y7x7eAlymUWbJiLSXv3IX1Z7uH1TYrNK"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9uFPgFJsx92"
      },
      "source": [
        "%%capture\n",
        "#download GREEK hunspell dict\n",
        "! gdown --id 1zPW8BtFY96STbDOUpGnnq6EcHJq8P3uP"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DGP8t2sEtCZa"
      },
      "source": [
        "import tarfile\n",
        "my_tar = tarfile.open('Greek_hunspell.tar.bz2')\n",
        "my_tar.extractall('/content') # specify which folder to extract to\n",
        "my_tar.close()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7sTuuLmtGN7"
      },
      "source": [
        "# make directory for elerrant resources and copy hunspell dict in\n",
        "!mkdir errant/errant/el \n",
        "!mkdir errant/errant/el/resources\n",
        "!cp /content/20110903/el_GR.dic errant/errant/el/resources/el_GR.txt"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCb0oxErucZb"
      },
      "source": [
        "%%capture\n",
        "! pip install greek-stemmer\n",
        "! python -m spacy download el"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IRtdfgjYupSq",
        "outputId": "dbefd135-ee39-4223-d7d9-b6a549d3eba3"
      },
      "source": [
        "%%capture\n",
        "# Instead of this classifier /content/errant/errant/en/classifier.py\n",
        "# I want to use this one and make sure that it can correct greek sentences\n",
        "! gdown --id 1glBOnTRSQeGLAXPjpIeSsxhG5qZkWLx3\n",
        "\n",
        "# move to the right location\n",
        "!mv gr_classifier.py errant/errant/el/classifier.py\n",
        "# move en.merger to el.\n",
        "!cp errant/errant/en/merger.py errant/errant/el/"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1glBOnTRSQeGLAXPjpIeSsxhG5qZkWLx3\n",
            "To: /content/gr_classifier.py\n",
            "\r  0% 0.00/10.7k [00:00<?, ?B/s]\r100% 10.7k/10.7k [00:00<00:00, 16.5MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajDBB2m0vHX-"
      },
      "source": [
        "!cp -r errant/errant ./_errant\n",
        "!rm -r errant\n",
        "!mv _errant errant"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ufTwdQOlvMu8",
        "outputId": "1354d0e3-6ff5-4ceb-c371-66fb63ae3de1"
      },
      "source": [
        "%%writefile errant/__init__.py\n",
        "\n",
        "from importlib import import_module\n",
        "import spacy\n",
        "from errant.annotator import Annotator\n",
        "\n",
        "\n",
        "# ERRANT version\n",
        "__version__ = '2.2.3'\n",
        "\n",
        "# Load an ERRANT Annotator object for a given language\n",
        "def load(lang, nlp=None):\n",
        "    # Make sure the language is supported\n",
        "    supported = {\"en\", \"el\"}\n",
        "    if lang not in supported:\n",
        "        raise Exception(\"%s is an unsupported or unknown language\" % lang)\n",
        "\n",
        "    # Load spacy\n",
        "    nlp = nlp or spacy.load(lang, disable=[\"ner\"])\n",
        "\n",
        "    # Load language edit merger\n",
        "    merger = import_module(\"errant.%s.merger\" % lang)\n",
        "\n",
        "    # Load language edit classifier\n",
        "    classifier = import_module(\"errant.%s.classifier\" % lang)\n",
        "    # The English classifier needs spacy\n",
        "    # Also adding the Greek classifier\n",
        "    if lang in {\"en\", \"el\"}: classifier.nlp = nlp\n",
        "\n",
        "    # Return a configured ERRANT annotator\n",
        "    return Annotator(lang, nlp, merger, classifier)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting errant/__init__.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2m7ndtwIvPaL",
        "outputId": "ec62079b-0341-47b8-c553-a87a718cbc81"
      },
      "source": [
        "!pip install python-Levenshtein\n",
        "from importlib import import_module\n",
        "import spacy\n",
        "from errant import Annotator\n",
        "\n",
        "# Load an ERRANT Annotator object for a given language\n",
        "def load(lang, nlp=None):\n",
        "    # Make sure the language is supported\n",
        "    supported = {\"en\", \"el\"}\n",
        "    if lang not in supported:\n",
        "        raise Exception(\"%s is an unsupported or unknown language\" % lang)\n",
        "\n",
        "    # Load spacy\n",
        "    nlp = nlp or spacy.load(lang, disable=[\"ner\"])\n",
        "\n",
        "    # Load language edit merger\n",
        "    merger = import_module(\"errant.%s.merger\" % lang)\n",
        "\n",
        "    # Load language edit classifier\n",
        "    classifier = import_module(\"errant.%s.classifier\" % lang)\n",
        "    # The English classifier needs spacy\n",
        "    # Also adding the Greek classifier\n",
        "    if lang in {\"en\", \"el\"}: classifier.nlp = nlp\n",
        "\n",
        "    # Return a configured ERRANT annotator\n",
        "    return Annotator(lang, nlp, merger, classifier)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: python-Levenshtein in /usr/local/lib/python3.7/dist-packages (0.12.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from python-Levenshtein) (57.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JX-PE_30veF8",
        "outputId": "3caa5919-e4f5-4b73-d2db-1227b6aefa62"
      },
      "source": [
        "%%writefile errant/el/classifier.py\n",
        "import re\n",
        "from pathlib import Path\n",
        "import Levenshtein\n",
        "#use greek stemmer https://pypi.org/project/greek-stemmer/\n",
        "from greek_stemmer import GreekStemmer\n",
        "import spacy\n",
        "import spacy.symbols as POS\n",
        "\n",
        "# Load Greek Hunspell word list\n",
        "def load_word_list(path):\n",
        "    with open(path) as word_list:\n",
        "        return set([word.strip() for word in word_list])\n",
        "\n",
        "\n",
        "\n",
        "# Classifier resources\n",
        "base_dir = \"errant/errant/en\"\n",
        "# Spacy\n",
        "nlp = None\n",
        "# Greek Stemmer\n",
        "stemmer = GreekStemmer()\n",
        "# Greek Word list\n",
        "spell = load_word_list('/content/errant/el/resources/el_GR.txt')\n",
        "\n",
        "# Rare POS tags that make uninformative error categories\n",
        "rare_pos = {\"INTJ\", \"NUM\", \"SYM\", \"X\"}\n",
        "# Open class coarse Spacy POS tags \n",
        "open_pos1 = {POS.ADJ, POS.ADV, POS.NOUN, POS.VERB}\n",
        "# Open class coarse Spacy POS tags (strings)\n",
        "open_pos2 = {\"ADJ\", \"ADV\", \"NOUN\", \"VERB\"}\n",
        "# POS tags with inflectional morphology\n",
        "inflected_tags = {\"ADJ\", \"ADV\", \"AUX\", \"DET\", \"PRON\", \"PROPN\", \"NOUN\", \"VERB\"}\n",
        "# Some dep labels that map to pos tags.\n",
        "dep_map = {\"ac\": \"ADP\", \"svp\": \"ADP\",\t\"punct\": \"PUNCT\", \"CCONJ\": \"CONJ\" }\n",
        "# Accents/Vowels\n",
        "accents=['ά','έ', 'ή', 'ί', 'ό', 'ύ', 'ώ']\n",
        "#Simplified cats\n",
        "simple_cats={'CCONJ':'CONJ', 'SCONJ':'CONJ', 'ADP':'PREP' }\n",
        "\n",
        "\n",
        "# Input: An Edit object\n",
        "# Output: The same Edit object with an updated error type\n",
        "def classify(edit):  \n",
        "    # Nothing to nothing is a detected but not corrected edit\n",
        "    if not edit.o_toks and not edit.c_toks:\n",
        "        edit.type = \"UNK\"\n",
        "    # Missing\n",
        "    elif not edit.o_toks and edit.c_toks:\n",
        "        op = \"M:\"\n",
        "        cat = simplify(get_one_sided_type(edit.c_toks))\n",
        "        edit.type = op+cat   \n",
        "    # Unnecessary\n",
        "    elif edit.o_toks and not edit.c_toks:\n",
        "        op = \"U:\"\n",
        "        cat = simplify(get_one_sided_type(edit.o_toks))\n",
        "        edit.type = op+cat\n",
        "    # Replacement and special cases\n",
        "    else:\n",
        "        # Same to same is a detected but not corrected edit\n",
        "        if edit.o_str == edit.c_str:\n",
        "            edit.type = \"UNK\"\n",
        "        # Classify the edit as if the last token wasn't there\n",
        "        elif edit.o_toks[-1].lower == edit.c_toks[-1].lower and \\\n",
        "                (len(edit.o_toks) > 1 or len(edit.c_toks) > 1):\n",
        "            # Store a copy of the full orig and cor toks\n",
        "            all_o_toks = edit.o_toks[:]\n",
        "            all_c_toks = edit.c_toks[:]\n",
        "            # Truncate the instance toks for classification\n",
        "            edit.o_toks = edit.o_toks[:-1]\n",
        "            edit.c_toks = edit.c_toks[:-1]\n",
        "            # Classify the truncated edit\n",
        "            edit = classify(edit)\n",
        "            # Restore the full orig and cor toks\n",
        "            edit.o_toks = all_o_toks\n",
        "            edit.c_toks = all_c_toks\n",
        "\n",
        "        # Accent/Final Nu special cases\n",
        "        #these need to go to replacement\n",
        "        elif accent(edit.o_toks, edit.c_toks) == \"miss_acc\":\n",
        "          edit.type = \"M:ACC\"\n",
        "        elif accent(edit.o_toks, edit.c_toks) == \"unn_acc\":\n",
        "          edit.type = \"U:ACC\"\n",
        "        elif final_n(edit.o_toks, edit.c_toks) == 'unn_fn':\n",
        "          edit.type = \"U:FN\"\n",
        "        elif final_n(edit.o_toks, edit.c_toks) == 'miss_fn':\n",
        "          edit.type = \"M:FN\"\n",
        "        # Replacement\n",
        "        else:\n",
        "            op = \"R:\"\n",
        "            cat = simplify(get_two_sided_type(edit.o_toks, edit.c_toks))\n",
        "            edit.type = op+cat\n",
        "    return edit\n",
        "\n",
        "\n",
        "\n",
        "# Input: Spacy tokens\n",
        "# Output: A list of pos and dep tag strings\n",
        "def get_edit_info(toks):\n",
        "    pos = []\n",
        "    dep = []\n",
        "    for tok in toks:\n",
        "        pos.append(tok.tag_)\n",
        "        dep.append(tok.dep_)\n",
        "    return pos, dep\n",
        "\n",
        "# Input: Spacy tokens\n",
        "# Output: An error type string based on input tokens from orig or cor\n",
        "# When one side of the edit is null, we can only use the other side\n",
        "def get_one_sided_type(toks):\n",
        "    # Special cases\n",
        "    if len(toks) == 1:\n",
        "        # Subjunctive \"να\" is treated as part of a verb form\n",
        "        if toks[0].lower_ == \"να\" and toks[0].pos == POS.PART :\n",
        "            return \"VERB:FORM\"     \n",
        "        \n",
        "    # Extract pos tags and parse info from the toks\n",
        "    pos_list, dep_list = get_edit_info(toks)\n",
        "    # Auxiliary verbs e.g \"έχω, είχα\" \n",
        "        # Μέλλοντας \"θα\"\n",
        "    if toks[0].lower_ == \"θα\" and toks[0].pos == POS.PART :\n",
        "        return \"VERB:TENSE\"\n",
        "    if toks[0].pos == POS.VERB and set(dep_list).issubset({\"aux\", \"auxpass\", \"obj\", \"advmod\"}):\n",
        "        return \"VERB:TENSE\"\n",
        "    # POS-based tags. Ignores rare, uninformative categories\n",
        "    if len(set(pos_list)) == 1 and pos_list[0] not in rare_pos:\n",
        "        return pos_list[0]\n",
        "    # More POS-based tags using special dependency labels\n",
        "    if len(set(dep_list)) == 1 and dep_list[0] in dep_map.keys():\n",
        "        return dep_map[dep_list[0]]\n",
        "    # Tricky cases\n",
        "    else:\n",
        "        return \"OTHER\"\n",
        "\n",
        "\n",
        "# Input 1: Spacy orig tokens\n",
        "# Input 2: Spacy cor tokens\n",
        "# Output: An error type string based on orig AND cor\n",
        "def get_two_sided_type(o_toks, c_toks):\n",
        "    # Extract pos tags and parse info from the toks as lists\n",
        "    o_pos, o_dep = get_edit_info(o_toks)\n",
        "    c_pos, c_dep = get_edit_info(c_toks)\n",
        "\n",
        "    # Orthography; i.e. whitespace and/or case errors.\n",
        "    if only_orth_change(o_toks, c_toks):\n",
        "        return \"ORTH\"\n",
        "    # Word Order; only matches exact reordering.\n",
        "    if exact_reordering(o_toks, c_toks):\n",
        "        return \"WO\"   \n",
        "             \n",
        "\n",
        "# 2. SPELLING AND INFLECTION\n",
        "        # Only check alphabetical strings on the original side\n",
        "        # Spelling errors take precedence over POS errors; this rule is ordered\n",
        "        if o_toks[0].text.isalpha():          \n",
        "            # Check a greek dict for both orig and lower case.            \n",
        "            if o_toks[0].text not in spell and \\\n",
        "                    o_toks[0].lower_ not in spell:\n",
        "                # Check if both sides have a common lemma\n",
        "                if o_toks[0].lemma == c_toks[0].lemma:\n",
        "                    # Inflection\n",
        "                    # Spacy issue returns nonetype when does not properly assign pos\n",
        "                    if o_pos == c_pos and o_pos[0] in {\"NOUN\", \"ADJ\", \"ADV\", \"PRON\", \"VERB\"}:\n",
        "                        return o_pos[0]+\":FORM\"\n",
        "                    # Unknown morphology; i.e. we cannot be more specific.\n",
        "                    else:\n",
        "                        return \"MORPH\"\n",
        "                # Use string similarity to detect true spelling errors.\n",
        "                else:\n",
        "                  char_ratio = Levenshtein.ratio(o_toks[0].text, c_toks[0].text)\n",
        "                  # Ratio > 0.5 means both side share at least half the same chars.\n",
        "                  # WARNING: THIS IS AN APPROXIMATION.\n",
        "                  if char_ratio > 0.5:\n",
        "                      return \"SPELL\"\n",
        "                  # If ratio is <= 0.5, the error is more complex\n",
        "                  else:\n",
        "                      # If POS is the same, this takes precedence over spelling.\n",
        "                      if o_pos == c_pos and \\\n",
        "                              o_pos[0] not in rare_pos:\n",
        "                          return o_pos[0]\n",
        "                      # Tricky cases.\n",
        "                      \n",
        "                      if char_ratio > 0.9:\n",
        "                        if o_toks[0] not in conts and c_toks[0] not in conts and \\\n",
        "                        accent(o_toks,c_toks) == \"repl_acc\":\n",
        "                          return \"ACC\"\n",
        "                      else:\n",
        "                        return \"OTHER\"   \n",
        "\n",
        "        # 3. MORPHOLOGY\n",
        "        # Only ADJ, ADV, NOUN and VERB can have inflectional changes.\n",
        "        if o_toks[0].lemma == c_toks[0].lemma and \\\n",
        "                o_pos[0] in open_pos2 and \\\n",
        "                c_pos[0] in open_pos2:\n",
        "            # Same POS on both sides\n",
        "            if o_pos == c_pos:\n",
        "                # Adjective form; e.g. comparatives\n",
        "                if o_pos[0] == \"ADJ\":\n",
        "                    return \"ADJ:FORM\"\n",
        "                # Noun number\n",
        "                if o_pos[0] == \"NOUN\":\n",
        "                    return \"NOUN:FORM\"\n",
        "                # Verbs - various types\n",
        "                if o_pos[0] == \"VERB\":\n",
        "                    # NOTE: These rules are carefully ordered.\n",
        "                    # Use the dep parse to find some form errors.\n",
        "                    # Main verbs preceded by aux cannot be tense or SVA.\n",
        "                    if preceded_by_aux(o_toks, c_toks):\n",
        "                        return \"VERB:FORM\"\n",
        "                if o_pos == c_pos and o_pos[0] == \"VERB\":\n",
        "                        return \"VERB:FORM\"\n",
        "                        \n",
        "\n",
        "        # 4. GENERAL\n",
        "        # Auxiliaries with different lemmas\n",
        "        if o_dep[0].startswith(\"aux\") and c_dep[0].startswith(\"aux\"):\n",
        "            return \"VERB:TENSE\"\n",
        "        # POS-based tags. Some of these are context sensitive mispellings.\n",
        "        if o_pos == c_pos and o_pos[0] not in rare_pos:\n",
        "            return o_pos[0]\n",
        "        # Some dep labels map to POS-based tags.\n",
        "        if o_dep == c_dep and o_dep[0] in dep_map.keys():\n",
        "            return dep_map[o_dep[0]]\n",
        "        else:\n",
        "            return \"OTHER\"\n",
        "\n",
        "    # Multi-token replacements (uncommon)\n",
        "    # All auxiliaries\n",
        "    if set(o_dep+c_dep).issubset({\"aux\", \"auxpass\"}):\n",
        "        return \"VERB:TENSE\"\n",
        "    # All same POS\n",
        "    if len(set(o_pos+c_pos)) == 1:\n",
        "        # Final verbs with the same lemma are tense\n",
        "        if o_pos[0] == \"VERB\" and \\\n",
        "                o_toks[-1].lemma == c_toks[-1].lemma:\n",
        "            return \"VERB:TENSE\"\n",
        "        # POS-based tags.\n",
        "        elif o_pos[0] not in rare_pos:\n",
        "            return o_pos[0]\n",
        "    # All same special dep labels.\n",
        "    if len(set(o_dep+c_dep)) == 1 and \\\n",
        "            o_dep[0] in dep_map.keys():\n",
        "        return dep_map[o_dep[0]]\n",
        "    # Verbs with particles e.g., μην κάνεις\n",
        "    if set(o_pos+c_pos) == {\"PART\", \"VERB\"}:\n",
        "      \n",
        "        if o_toks[-1].lemma == c_toks[-1].lemma:\n",
        "            return \"VERB:FORM\"\n",
        "        # In case particle needs to go\n",
        "        else:\n",
        "            return \"VERB\"\n",
        "    # Tricky cases.\n",
        "    else:\n",
        "        return \"OTHER\"\n",
        "\n",
        "\n",
        "def only_orth_change(o_toks, c_toks):\n",
        "    o_join = \"\".join([o.lower_ for o in o_toks])\n",
        "    c_join = \"\".join([c.lower_ for c in c_toks])\n",
        "    if o_join == c_join:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "# Input 1: Spacy orig tokens\n",
        "# Input 2: Spacy cor tokens\n",
        "# Output: Boolean; the tokens are exactly the same but in a different order\n",
        "def exact_reordering(o_toks, c_toks):\n",
        "    # Sorting lets us keep duplicates.\n",
        "    o_set = sorted([o.lower_ for o in o_toks])\n",
        "    c_set = sorted([c.lower_ for c in c_toks])\n",
        "    if o_set == c_set:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "\n",
        "\n",
        "def accent(o_toks, c_toks):\n",
        "  o_toks = str(o_toks)\n",
        "  c_toks = str(c_toks)\n",
        "  o_chars =[char for char in o_toks]\n",
        "  c_chars = [char for char in c_toks]\n",
        "   \n",
        "  char_list1=[]  \n",
        "  char_list2=[]  \n",
        "\n",
        "  if set(o_chars).isdisjoint(accents) == False and set(c_chars).isdisjoint(accents) == True:\n",
        "    return \"unn_acc\"\n",
        "  elif set(o_chars).isdisjoint(accents) == True and set(c_chars).isdisjoint(accents) == False:\n",
        "   return \"miss_acc\"\n",
        "  elif set(o_chars).isdisjoint(accents) == False and set(c_chars).isdisjoint(accents) == False:\n",
        "    char1 = list(set(o_chars).intersection(accents))\n",
        "    char2 = list(set(c_chars).intersection(accents))\n",
        "    if len(char2)>1:\n",
        "      return 'miss_acc'\n",
        "    elif len(char1)>1:\n",
        "      return 'unn_acc'\n",
        "    else:\n",
        "      len(char1) == len(char2)\n",
        "      if o_chars.index(char1[0]) != c_chars.index(char2[0]): \n",
        "        return'repl_acc'\n",
        "\n",
        "    \n",
        "def final_n(o_toks, c_toks):\n",
        "  o_toks = str(o_toks)\n",
        "  c_toks = str(c_toks)\n",
        "  o_chars =[char for char in o_toks]\n",
        "  c_chars = [char for char in c_toks]\n",
        "\n",
        "  if o_chars == c_chars[:-1] and c_chars[-1]=='ν':\n",
        "    return \"miss_fn\"\n",
        "  elif o_chars[:-1] == c_chars and o_chars[-1]=='ν':\n",
        "    return \"unn_fn\"\n",
        "\n",
        "def simplify(cat):\n",
        "  if cat in simple_cats:\n",
        "    cat = simple_cats.get(cat)\n",
        "  return cat"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting errant/el/classifier.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YhwkEcgf0FaW"
      },
      "source": [
        "import errant\n",
        "import argparse\n",
        "from contextlib import ExitStack\n",
        "import errant\n",
        "\n",
        "def parallel(out, orig, cor, tok = False, merge ='rules', lev = False ):\n",
        "  print(\"Loading resources...\")\n",
        "  # Load Errant\n",
        "  annotator = errant.load(\"en\")\n",
        "  # Open output m2 file\n",
        "  out_m2 = open(out, \"w\")\n",
        "\n",
        "  print(\"Processing parallel files...\")\n",
        "  # Process an arbitrary number of files line by line simultaneously. Python 3.3+\n",
        "  # See https://tinyurl.com/y4cj4gth\n",
        "  with ExitStack() as stack:\n",
        "      in_files = [stack.enter_context(open(i)) for i in [orig]+cor]\n",
        "      # Process each line of all input files\n",
        "      for line in zip(*in_files):\n",
        "          # Get the original and all the corrected texts\n",
        "          orig = line[0].strip()\n",
        "          cors = line[1:]\n",
        "          # Skip the line if orig is empty\n",
        "          if not orig: continue\n",
        "          # Parse orig with spacy\n",
        "          orig = annotator.parse(orig, tok)\n",
        "          # Write orig to the output m2 file\n",
        "          out_m2.write(\" \".join([\"S\"]+[token.text for token in orig])+\"\\n\")\n",
        "          # Loop through the corrected texts\n",
        "          for cor_id, cor in enumerate(cors):\n",
        "              cor = cor.strip()\n",
        "              # If the texts are the same, write a noop edit\n",
        "              if orig.text.strip() == cor:\n",
        "                  out_m2.write(noop_edit(cor_id)+\"\\n\")\n",
        "              # Otherwise, do extra processing\n",
        "              else:\n",
        "                  # Parse cor with spacy\n",
        "                  cor = annotator.parse(cor, tok)\n",
        "                  # Align the texts and extract and classify the edits\n",
        "                  edits = annotator.annotate(orig, cor, lev, merge)\n",
        "                  # Loop through the edits\n",
        "                  for edit in edits:\n",
        "                      # Write the edit to the output m2 file\n",
        "                      out_m2.write(edit.to_m2(cor_id)+\"\\n\")\n",
        "          # Write a newline when we have processed all corrections for each line\n",
        "          out_m2.write(\"\\n\")\n",
        "          \n",
        "# Input: A coder id\n",
        "# Output: A noop edit; i.e. text contains no edits\n",
        "def noop_edit(id=0):\n",
        "    return \"A -1 -1|||noop|||-NONE-|||REQUIRED|||-NONE-|||\"+str(id)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bvMJ2apNVEj"
      },
      "source": [
        "parallel('out_m2','orig.txt',['corr.txt'])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
