{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "elerrant_LT.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "296ON-yTHgg8"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/katkorre/elerrant/blob/main/elerrant_LT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/katkorre/elerrant.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BgDcHOr6X0k1",
        "outputId": "d4b7a8fc-a891-496c-fe3b-c7cb1accca11"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'elerrant' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EL.ERRANT (simply run this cell to use the tool)\n",
        "\n",
        "\n",
        "*   ERRANT is to automatically annotate parallel sentences with error type information.\n",
        "**Example** : **Original**: This are gramamtical sentence . **Corrected** : This is a grammatical sentence . ** Output M2** : `S This are gramamtical sentence.` \n",
        "`A 1 2|||R:VERB:SVA|||is|||REQUIRED|||-NONE-|||0 A 2 2|||M:DET|||a|||REQUIRED|||-NONE-|||0 A 2 3|||R:SPELL|||grammatical|||REQUIRED|||-NONE-|||0 A -1 -1|||noop|||-NONE-|||REQUIRED|||-NONE-|||1`\n",
        "\n"
      ],
      "metadata": {
        "id": "l2wvAPiFGVdJ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_MVSoFBBr4n3"
      },
      "source": [
        "# cloning original errant\n",
        "#! git clone https://github.com/chrisjbryant/errant.git --branch 2.3.0"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/chrisjbryant/errant.git --branch v2.3.0 --single-branch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QoWxlwLAxWY4",
        "outputId": "e906acc1-65da-4aa6-da07-1bdf35157dd3"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'errant'...\n",
            "remote: Enumerating objects: 259, done.\u001b[K\n",
            "remote: Counting objects: 100% (125/125), done.\u001b[K\n",
            "remote: Compressing objects: 100% (39/39), done.\u001b[K\n",
            "remote: Total 259 (delta 95), reused 86 (delta 86), pack-reused 134\u001b[K\n",
            "Receiving objects: 100% (259/259), 1.10 MiB | 3.57 MiB/s, done.\n",
            "Resolving deltas: 100% (145/145), done.\n",
            "Note: checking out '9111c6c5ca0dffdd5d8023faab91cc94aa3aef93'.\n",
            "\n",
            "You are in 'detached HEAD' state. You can look around, make experimental\n",
            "changes and commit them, and you can discard any commits you make in this\n",
            "state without impacting any branches by performing another checkout.\n",
            "\n",
            "If you want to create a new branch to retain commits you create, you may\n",
            "do so (now or later) by using -b with the checkout command again. Example:\n",
            "\n",
            "  git checkout -b <new-branch-name>\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9uFPgFJsx92",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f33931c4-4608-4772-b9ff-17dc2ac043df"
      },
      "source": [
        "# download GREEK hunspell dict\n",
        "! npx degit katkorre/elerrant/resources/el_GR.txt el_GR.txt"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l[..................] / rollbackFailedOptional: verb npm-session ffc879d8a77a2fa\u001b[0m\u001b[K\r[#######...........] \\ extract:degit: verb lock using /root/.npm/_locks/staging\u001b[0m\u001b[K\r[############......] / refresh-package-json:degit: timing action:finalize Compl\u001b[0m\u001b[K\r\r\u001b[K\u001b[?25hnpx: installed 1 in 1.721s\n",
            "\u001b[31m! ENOTDIR: not a directory, scandir 'el_GR.txt'\u001b[39m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7sTuuLmtGN7"
      },
      "source": [
        "# make directory for elerrant resources and move hunspell dict in\n",
        "!mkdir errant/errant/el \n",
        "!mkdir errant/errant/el/resources\n",
        "!mv /content/el_GR.txt errant/errant/el/resources/el_GR.txt"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCb0oxErucZb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48a0f0ef-4dd7-495f-cda1-197b2eb29804"
      },
      "source": [
        "# install greek-stemmer, greek spacy, and levenshtein distance\n",
        "! pip install greek-stemmer\n",
        "! python -m spacy download el\n",
        "! pip install python-Levenshtein\n",
        "#!pip install rapidfuzz\n"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: greek-stemmer in /usr/local/lib/python3.7/dist-packages (0.1.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from greek-stemmer) (3.13)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from greek-stemmer) (0.16.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting el_core_news_sm==2.2.5\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/el_core_news_sm-2.2.5/el_core_news_sm-2.2.5.tar.gz (11.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 11.4 MB 10.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from el_core_news_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->el_core_news_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->el_core_news_sm==2.2.5) (57.4.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->el_core_news_sm==2.2.5) (1.21.6)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->el_core_news_sm==2.2.5) (0.9.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->el_core_news_sm==2.2.5) (4.64.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->el_core_news_sm==2.2.5) (1.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->el_core_news_sm==2.2.5) (3.0.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->el_core_news_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->el_core_news_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->el_core_news_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->el_core_news_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->el_core_news_sm==2.2.5) (2.0.6)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->el_core_news_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->el_core_news_sm==2.2.5) (4.11.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->el_core_news_sm==2.2.5) (3.8.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->el_core_news_sm==2.2.5) (4.2.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->el_core_news_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->el_core_news_sm==2.2.5) (2022.5.18.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->el_core_news_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->el_core_news_sm==2.2.5) (1.24.3)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('el_core_news_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/el_core_news_sm -->\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/data/el\n",
            "You can now load the model via spacy.load('el')\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: python-Levenshtein in /usr/local/lib/python3.7/dist-packages (0.12.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from python-Levenshtein) (57.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajDBB2m0vHX-"
      },
      "source": [
        "!cp errant/errant/en/merger.py errant/errant/el/\n",
        "!cp -r errant/errant ./_errant\n",
        "!rm -r errant\n",
        "!mv _errant errant"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ufTwdQOlvMu8",
        "outputId": "5d733d47-d6ea-4588-c71b-3a6216dda9cf"
      },
      "source": [
        "%%writefile errant/__init__.py\n",
        "import Levenshtein\n",
        "from importlib import import_module\n",
        "import spacy\n",
        "from errant.annotator import Annotator\n",
        "\n",
        "\n",
        "# ERRANT version\n",
        "__version__ = '2.2.3'\n",
        "\n",
        "# Load an ERRANT Annotator object for a given language\n",
        "def load(lang, nlp=None):\n",
        "    # Make sure the language is supported\n",
        "    supported = {\"en\", \"el\"}\n",
        "    if lang not in supported:\n",
        "        raise Exception(\"%s is an unsupported or unknown language\" % lang)\n",
        "\n",
        "    # Load spacy\n",
        "    nlp = nlp or spacy.load(lang, disable=[\"ner\"])\n",
        "\n",
        "    # Load language edit merger\n",
        "    merger = import_module(\"errant.%s.merger\" % lang)\n",
        "\n",
        "    # Load language edit classifier\n",
        "    classifier = import_module(\"errant.%s.classifier\" % lang)\n",
        "    # The English classifier needs spacy\n",
        "    # Also adding the Greek classifier\n",
        "    if lang in {\"en\", \"el\"}: classifier.nlp = nlp\n",
        "\n",
        "    # Return a configured ERRANT annotator\n",
        "    return Annotator(lang, nlp, merger, classifier)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting errant/__init__.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JX-PE_30veF8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77383074-6945-43c2-eb5a-b82d9cf88f0b"
      },
      "source": [
        "%%writefile errant/el/classifier.py\n",
        "import re\n",
        "from pathlib import Path\n",
        "#from rapidfuzz.distance import Levenshtein\n",
        "import Levenshtein\n",
        "#use greek stemmer https://pypi.org/project/greek-stemmer/\n",
        "from greek_stemmer import GreekStemmer\n",
        "import spacy\n",
        "import spacy.symbols as POS\n",
        "\n",
        "# Load Greek Hunspell word list\n",
        "def load_word_list(path):\n",
        "    with open(path) as word_list:\n",
        "        return set([word.strip() for word in word_list])\n",
        "\n",
        "\n",
        "\n",
        "# Classifier resources\n",
        "base_dir = \"errant/errant/en\"\n",
        "# Spacy\n",
        "nlp = None\n",
        "# Greek Stemmer\n",
        "stemmer = GreekStemmer()\n",
        "# Greek Word list\n",
        "spell = load_word_list('/content/errant/el/resources/el_GR.txt')\n",
        "\n",
        "# Rare POS tags that make uninformative error categories\n",
        "rare_pos = {\"INTJ\", \"NUM\", \"SYM\", \"X\"}\n",
        "# Open class coarse Spacy POS tags \n",
        "open_pos1 = {POS.ADJ, POS.ADV, POS.NOUN, POS.VERB}\n",
        "# Open class coarse Spacy POS tags (strings)\n",
        "open_pos2 = {\"ADJ\", \"ADV\", \"NOUN\", \"VERB\"}\n",
        "# POS tags with inflectional morphology\n",
        "inflected_tags = {\"ADJ\", \"ADV\", \"AUX\", \"DET\", \"PRON\", \"PROPN\", \"NOUN\", \"VERB\"}\n",
        "# Some dep labels that map to pos tags.\n",
        "dep_map = {\"ac\": \"ADP\", \"svp\": \"ADP\",\t\"punct\": \"PUNCT\", \"CCONJ\": \"CONJ\" }\n",
        "# Accents/Vowels\n",
        "accents=['ά','έ', 'ή', 'ί', 'ό', 'ύ', 'ώ']\n",
        "#Simplified cats\n",
        "simple_cats={'CCONJ':'CONJ', 'SCONJ':'CONJ', 'ADP':'PREP' }\n",
        "\n",
        "\n",
        "# Input: An Edit object\n",
        "# Output: The same Edit object with an updated error type\n",
        "def classify(edit):  \n",
        "    # Nothing to nothing is a detected but not corrected edit\n",
        "    if not edit.o_toks and not edit.c_toks:\n",
        "        edit.type = \"UNK\"\n",
        "    # Missing\n",
        "    elif not edit.o_toks and edit.c_toks:\n",
        "        op = \"M:\"\n",
        "        cat = simplify(get_one_sided_type(edit.c_toks))\n",
        "        edit.type = op+cat   \n",
        "    # Unnecessary\n",
        "    elif edit.o_toks and not edit.c_toks:\n",
        "        op = \"U:\"\n",
        "        cat = simplify(get_one_sided_type(edit.o_toks))\n",
        "        edit.type = op+cat\n",
        "    # Replacement and special cases\n",
        "    else:\n",
        "        # Same to same is a detected but not corrected edit\n",
        "        if edit.o_str == edit.c_str:\n",
        "            edit.type = \"UNK\"\n",
        "        # Classify the edit as if the last token wasn't there\n",
        "        elif edit.o_toks[-1].lower == edit.c_toks[-1].lower and \\\n",
        "                (len(edit.o_toks) > 1 or len(edit.c_toks) > 1):\n",
        "            # Store a copy of the full orig and cor toks\n",
        "            all_o_toks = edit.o_toks[:]\n",
        "            all_c_toks = edit.c_toks[:]\n",
        "            # Truncate the instance toks for classification\n",
        "            edit.o_toks = edit.o_toks[:-1]\n",
        "            edit.c_toks = edit.c_toks[:-1]\n",
        "            # Classify the truncated edit\n",
        "            edit = classify(edit)\n",
        "            # Restore the full orig and cor toks\n",
        "            edit.o_toks = all_o_toks\n",
        "            edit.c_toks = all_c_toks\n",
        "\n",
        "        # Accent/Final Nu special cases\n",
        "        #these need to go to replacement\n",
        "        elif accent(edit.o_toks, edit.c_toks) == \"miss_acc\":\n",
        "          edit.type = \"M:ACC\"\n",
        "        elif accent(edit.o_toks, edit.c_toks) == \"unn_acc\":\n",
        "          edit.type = \"U:ACC\"\n",
        "        elif final_n(edit.o_toks, edit.c_toks) == 'unn_fn':\n",
        "          edit.type = \"U:FN\"\n",
        "        elif final_n(edit.o_toks, edit.c_toks) == 'miss_fn':\n",
        "          edit.type = \"M:FN\"\n",
        "        # Replacement\n",
        "        else:\n",
        "            op = \"R:\"\n",
        "            cat = simplify(get_two_sided_type(edit.o_toks, edit.c_toks))\n",
        "            edit.type = op+cat\n",
        "    return edit\n",
        "\n",
        "\n",
        "\n",
        "# Input: Spacy tokens\n",
        "# Output: A list of pos and dep tag strings\n",
        "def get_edit_info(toks):\n",
        "    pos = []\n",
        "    dep = []\n",
        "    for tok in toks:\n",
        "        pos.append(tok.tag_)\n",
        "        dep.append(tok.dep_)\n",
        "    return pos, dep\n",
        "\n",
        "# Input: Spacy tokens\n",
        "# Output: An error type string based on input tokens from orig or cor\n",
        "# When one side of the edit is null, we can only use the other side\n",
        "def get_one_sided_type(toks):\n",
        "    # Special cases\n",
        "    if len(toks) == 1:\n",
        "        # Subjunctive \"να\" is treated as part of a verb form\n",
        "        if toks[0].lower_ == \"να\" and toks[0].pos == POS.PART :\n",
        "            return \"VERB:FORM\"     \n",
        "        \n",
        "    # Extract pos tags and parse info from the toks\n",
        "    pos_list, dep_list = get_edit_info(toks)\n",
        "    # Auxiliary verbs e.g \"έχω, είχα\" \n",
        "        # Μέλλοντας \"θα\"\n",
        "    if toks[0].lower_ == \"θα\" and toks[0].pos == POS.PART :\n",
        "        return \"VERB:TENSE\"\n",
        "    if toks[0].pos == POS.VERB and set(dep_list).issubset({\"aux\", \"auxpass\", \"obj\", \"advmod\"}):\n",
        "        return \"VERB:TENSE\"\n",
        "    # POS-based tags. Ignores rare, uninformative categories\n",
        "    if len(set(pos_list)) == 1 and pos_list[0] not in rare_pos:\n",
        "        return pos_list[0]\n",
        "    # More POS-based tags using special dependency labels\n",
        "    if len(set(dep_list)) == 1 and dep_list[0] in dep_map.keys():\n",
        "        return dep_map[dep_list[0]]\n",
        "    # Tricky cases\n",
        "    else:\n",
        "        return \"OTHER\"\n",
        "\n",
        "\n",
        "# Input 1: Spacy orig tokens\n",
        "# Input 2: Spacy cor tokens\n",
        "# Output: An error type string based on orig AND cor\n",
        "def get_two_sided_type(o_toks, c_toks):\n",
        "    # Extract pos tags and parse info from the toks as lists\n",
        "    o_pos, o_dep = get_edit_info(o_toks)\n",
        "    c_pos, c_dep = get_edit_info(c_toks)\n",
        "\n",
        "    # Orthography; i.e. whitespace and/or case errors.\n",
        "    if only_orth_change(o_toks, c_toks):\n",
        "        return \"ORTH\"\n",
        "    # Word Order; only matches exact reordering.\n",
        "    if exact_reordering(o_toks, c_toks):\n",
        "        return \"WO\"   \n",
        "             \n",
        "\n",
        "# 2. SPELLING AND INFLECTION\n",
        "        # Only check alphabetical strings on the original side\n",
        "        # Spelling errors take precedence over POS errors; this rule is ordered\n",
        "        if o_toks[0].text.isalpha():          \n",
        "            # Check a greek dict for both orig and lower case.            \n",
        "            if o_toks[0].text not in spell and \\\n",
        "                    o_toks[0].lower_ not in spell:\n",
        "                # Check if both sides have a common lemma\n",
        "                if o_toks[0].lemma == c_toks[0].lemma:\n",
        "                    # Inflection\n",
        "                    # Spacy issue returns nonetype when does not properly assign pos\n",
        "                    if o_pos == c_pos and o_pos[0] in {\"NOUN\", \"ADJ\", \"ADV\", \"PRON\", \"VERB\"}:\n",
        "                        return o_pos[0]+\":FORM\"\n",
        "                    # Unknown morphology; i.e. we cannot be more specific.\n",
        "                    else:\n",
        "                        return \"MORPH\"\n",
        "                # Use string similarity to detect true spelling errors.\n",
        "                else:\n",
        "                  char_ratio = Levenshtein.ratio(o_toks[0].text, c_toks[0].text)\n",
        "                  # Ratio > 0.5 means both side share at least half the same chars.\n",
        "                  # WARNING: THIS IS AN APPROXIMATION.\n",
        "                  if char_ratio > 0.5:\n",
        "                      return \"SPELL\"\n",
        "                  # If ratio is <= 0.5, the error is more complex\n",
        "                  else:\n",
        "                      # If POS is the same, this takes precedence over spelling.\n",
        "                      if o_pos == c_pos and \\\n",
        "                              o_pos[0] not in rare_pos:\n",
        "                          return o_pos[0]\n",
        "                      # Tricky cases.\n",
        "                      \n",
        "                      if char_ratio > 0.9:\n",
        "                        if o_toks[0] not in conts and c_toks[0] not in conts and \\\n",
        "                        accent(o_toks,c_toks) == \"repl_acc\":\n",
        "                          return \"ACC\"\n",
        "                      else:\n",
        "                        return \"OTHER\"   \n",
        "\n",
        "        # 3. MORPHOLOGY\n",
        "        # Only ADJ, ADV, NOUN and VERB can have inflectional changes.\n",
        "        if o_toks[0].lemma == c_toks[0].lemma and \\\n",
        "                o_pos[0] in open_pos2 and \\\n",
        "                c_pos[0] in open_pos2:\n",
        "            # Same POS on both sides\n",
        "            if o_pos == c_pos:\n",
        "                # Adjective form; e.g. comparatives\n",
        "                if o_pos[0] == \"ADJ\":\n",
        "                    return \"ADJ:FORM\"\n",
        "                # Noun number\n",
        "                if o_pos[0] == \"NOUN\":\n",
        "                    return \"NOUN:FORM\"\n",
        "                # Verbs - various types\n",
        "                if o_pos[0] == \"VERB\":\n",
        "                    # NOTE: These rules are carefully ordered.\n",
        "                    # Use the dep parse to find some form errors.\n",
        "                    # Main verbs preceded by aux cannot be tense or SVA.\n",
        "                    if preceded_by_aux(o_toks, c_toks):\n",
        "                        return \"VERB:FORM\"\n",
        "                if o_pos == c_pos and o_pos[0] == \"VERB\":\n",
        "                        return \"VERB:FORM\"\n",
        "                        \n",
        "\n",
        "        # 4. GENERAL\n",
        "        # Auxiliaries with different lemmas\n",
        "        if o_dep[0].startswith(\"aux\") and c_dep[0].startswith(\"aux\"):\n",
        "            return \"VERB:TENSE\"\n",
        "        # POS-based tags. Some of these are context sensitive mispellings.\n",
        "        if o_pos == c_pos and o_pos[0] not in rare_pos:\n",
        "            return o_pos[0]\n",
        "        # Some dep labels map to POS-based tags.\n",
        "        if o_dep == c_dep and o_dep[0] in dep_map.keys():\n",
        "            return dep_map[o_dep[0]]\n",
        "        else:\n",
        "            return \"OTHER\"\n",
        "\n",
        "    # Multi-token replacements (uncommon)\n",
        "    # All auxiliaries\n",
        "    if set(o_dep+c_dep).issubset({\"aux\", \"auxpass\"}):\n",
        "        return \"VERB:TENSE\"\n",
        "    # All same POS\n",
        "    if len(set(o_pos+c_pos)) == 1:\n",
        "        # Final verbs with the same lemma are tense\n",
        "        if o_pos[0] == \"VERB\" and \\\n",
        "                o_toks[-1].lemma == c_toks[-1].lemma:\n",
        "            return \"VERB:TENSE\"\n",
        "        # POS-based tags.\n",
        "        elif o_pos[0] not in rare_pos:\n",
        "            return o_pos[0]\n",
        "    # All same special dep labels.\n",
        "    if len(set(o_dep+c_dep)) == 1 and \\\n",
        "            o_dep[0] in dep_map.keys():\n",
        "        return dep_map[o_dep[0]]\n",
        "    # Verbs with particles e.g., μην κάνεις\n",
        "    if set(o_pos+c_pos) == {\"PART\", \"VERB\"}:\n",
        "      \n",
        "        if o_toks[-1].lemma == c_toks[-1].lemma:\n",
        "            return \"VERB:FORM\"\n",
        "        # In case particle needs to go\n",
        "        else:\n",
        "            return \"VERB\"\n",
        "    # Tricky cases.\n",
        "    else:\n",
        "        return \"OTHER\"\n",
        "\n",
        "\n",
        "def only_orth_change(o_toks, c_toks):\n",
        "    o_join = \"\".join([o.lower_ for o in o_toks])\n",
        "    c_join = \"\".join([c.lower_ for c in c_toks])\n",
        "    if o_join == c_join:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "# Input 1: Spacy orig tokens\n",
        "# Input 2: Spacy cor tokens\n",
        "# Output: Boolean; the tokens are exactly the same but in a different order\n",
        "def exact_reordering(o_toks, c_toks):\n",
        "    # Sorting lets us keep duplicates.\n",
        "    o_set = sorted([o.lower_ for o in o_toks])\n",
        "    c_set = sorted([c.lower_ for c in c_toks])\n",
        "    if o_set == c_set:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "\n",
        "\n",
        "def accent(o_toks, c_toks):\n",
        "  o_toks = str(o_toks)\n",
        "  c_toks = str(c_toks)\n",
        "  o_chars =[char for char in o_toks]\n",
        "  c_chars = [char for char in c_toks]\n",
        "   \n",
        "  char_list1=[]  \n",
        "  char_list2=[]  \n",
        "\n",
        "  if set(o_chars).isdisjoint(accents) == False and set(c_chars).isdisjoint(accents) == True:\n",
        "    return \"unn_acc\"\n",
        "  elif set(o_chars).isdisjoint(accents) == True and set(c_chars).isdisjoint(accents) == False:\n",
        "   return \"miss_acc\"\n",
        "  elif set(o_chars).isdisjoint(accents) == False and set(c_chars).isdisjoint(accents) == False:\n",
        "    char1 = list(set(o_chars).intersection(accents))\n",
        "    char2 = list(set(c_chars).intersection(accents))\n",
        "    if len(char2)>1:\n",
        "      return 'miss_acc'\n",
        "    elif len(char1)>1:\n",
        "      return 'unn_acc'\n",
        "    else:\n",
        "      len(char1) == len(char2)\n",
        "      if o_chars.index(char1[0]) != c_chars.index(char2[0]): \n",
        "        return'repl_acc'\n",
        "\n",
        "    \n",
        "def final_n(o_toks, c_toks):\n",
        "  o_toks = str(o_toks)\n",
        "  c_toks = str(c_toks)\n",
        "  o_chars =[char for char in o_toks]\n",
        "  c_chars = [char for char in c_toks]\n",
        "\n",
        "  if o_chars == c_chars[:-1] and c_chars[-1]=='ν':\n",
        "    return \"miss_fn\"\n",
        "  elif o_chars[:-1] == c_chars and o_chars[-1]=='ν':\n",
        "    return \"unn_fn\"\n",
        "\n",
        "def simplify(cat):\n",
        "  if cat in simple_cats:\n",
        "    cat = simple_cats.get(cat)\n",
        "  return cat"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing errant/el/classifier.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YhwkEcgf0FaW"
      },
      "source": [
        "import errant\n",
        "import argparse\n",
        "from contextlib import ExitStack\n",
        "import errant\n",
        "\n",
        "def parallel(out, orig, cor, tok = False, merge ='rules', lev = False ):\n",
        "  print(\"Loading resources...\")\n",
        "  # Load Errant\n",
        "  annotator = errant.load(\"en\")\n",
        "  # Open output m2 file\n",
        "  out_m2 = open(out, \"w\")\n",
        "\n",
        "  print(\"Processing parallel files...\")\n",
        "  # Process an arbitrary number of files line by line simultaneously. Python 3.3+\n",
        "  # See https://tinyurl.com/y4cj4gth\n",
        "  with ExitStack() as stack:\n",
        "      in_files = [stack.enter_context(open(i)) for i in [orig]+cor]\n",
        "      # Process each line of all input files\n",
        "      for line in zip(*in_files):\n",
        "          # Get the original and all the corrected texts\n",
        "          orig = line[0].strip()\n",
        "          cors = line[1:]\n",
        "          # Skip the line if orig is empty\n",
        "          if not orig: continue\n",
        "          # Parse orig with spacy\n",
        "          orig = annotator.parse(orig, tok)\n",
        "          # Write orig to the output m2 file\n",
        "          out_m2.write(\" \".join([\"S\"]+[token.text for token in orig])+\"\\n\")\n",
        "          # Loop through the corrected texts\n",
        "          for cor_id, cor in enumerate(cors):\n",
        "              cor = cor.strip()\n",
        "              # If the texts are the same, write a noop edit\n",
        "              if orig.text.strip() == cor:\n",
        "                  out_m2.write(noop_edit(cor_id)+\"\\n\")\n",
        "              # Otherwise, do extra processing\n",
        "              else:\n",
        "                  # Parse cor with spacy\n",
        "                  cor = annotator.parse(cor, tok)\n",
        "                  # Align the texts and extract and classify the edits\n",
        "                  edits = annotator.annotate(orig, cor, lev, merge)\n",
        "                  # Loop through the edits\n",
        "                  for edit in edits:\n",
        "                      # Write the edit to the output m2 file\n",
        "                      out_m2.write(edit.to_m2(cor_id)+\"\\n\")\n",
        "          # Write a newline when we have processed all corrections for each line\n",
        "          out_m2.write(\"\\n\")\n",
        "          \n",
        "# Input: A coder id\n",
        "# Output: A noop edit; i.e. text contains no edits\n",
        "def noop_edit(id=0):\n",
        "    return \"A -1 -1|||noop|||-NONE-|||REQUIRED|||-NONE-|||\"+str(id)"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create text files for annotation\n",
        "\n",
        "*   There must be a file for the original sentences and a separate file for the corrections.\n",
        "\n"
      ],
      "metadata": {
        "id": "296ON-yTHgg8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The original\n",
        "with open(\"orig.txt\", \"w\") as o:\n",
        "  o.write(\"Καλες διακωπές\")\n",
        "\n",
        "# The corrected\n",
        "with open(\"corr.txt\", \"w\") as o:\n",
        "  o.write(\"Καλές διακοπές .\")"
      ],
      "metadata": {
        "id": "-wRW2M789TRx"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Then employ `elerrant` to read the found errors:"
      ],
      "metadata": {
        "id": "fWhkj3rs-KUp"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bvMJ2apNVEj"
      },
      "source": [
        "%%capture\n",
        "parallel('out_m2','orig.txt',['corr.txt'])"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Flag explanations:\n",
        "\n",
        "`(1)A 1 2|||(2)R:VERB|||(3)είναι|||(4)REQUIRED|||(5)-NONE-|||(6)0' \n",
        "\n",
        "1. Error spans (which tokens convey the error)\n",
        "2. Error type\n",
        "3. Correction\n",
        "4. Required or not required field(not particular use)\n",
        "5. -NONE- is also a not interpretable flag\n",
        "6. Annotators id \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KI3FnBbW-Ux8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "open(\"out_m2\").read()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "yI2fvNjS9tzy",
        "outputId": "efc17418-14a9-4168-b153-2a9364164e12"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'S Καλες διακωπές\\nA 0 1|||R:SPELL|||Καλές|||REQUIRED|||-NONE-|||0\\nA 1 2|||R:SPELL|||διακοπές|||REQUIRED|||-NONE-|||0\\nA 2 2|||M:PUNCT|||.|||REQUIRED|||-NONE-|||0\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GNC example"
      ],
      "metadata": {
        "id": "iQke6FpVYN3i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "parallel('gnc_m2','/content/elerrant/GNC/orig.txt',['/content/elerrant/GNC/corr.txt'])"
      ],
      "metadata": {
        "id": "LR_bzCx_bDun"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using the API"
      ],
      "metadata": {
        "id": "RHLys0eyP8gY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "c-YdWqhmRrgQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ann = pd.read_excel('/content/elerrant/GNC/GNC_annotator_A.xlsx')"
      ],
      "metadata": {
        "id": "Sa7n8csVQpwG"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ann.head()"
      ],
      "metadata": {
        "id": "qndhMyajQ0e6",
        "outputId": "6d5f7449-e607-4b64-d3b2-60af33b7540b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  Label                                      Original Text  \\\n",
              "0     e  Σίγουρα η ραγδαία εξέλιξη των Επιστημών αλλάζο...   \n",
              "1     e  Αυτό αναπόφευκτα δημιουργεί νέα κοινωνικά και ...   \n",
              "2     e  Αυτό αναπόφευκτα δημιουργεί νέα κοινωνικά και ...   \n",
              "3     e  Αυτό αναπόφευκτα δημιουργεί νέα κοινωνικά και ...   \n",
              "4     c  Επομένως, οι επιφυλάξεις του σύγχρονου ανθρώπο...   \n",
              "\n",
              "                                      Corrected Text Error Description  \\\n",
              "0  Σίγουρα η ραγδαία εξέλιξη των Επιστημών αλλάζε...                 R   \n",
              "1  Αυτό αναπόφευκτα δημιουργεί νέα κοινωνικά και ...                 R   \n",
              "2  Αυτό αναπόφευκτα δημιουργεί νέα κοινωνικά και ...                 M   \n",
              "3  Αυτό αναπόφευκτα δημιουργεί νέα κοινωνικά και ...                 U   \n",
              "4                                                NaN               NaN   \n",
              "\n",
              "  Error Type  Fluency  \n",
              "0   VERB:SVA      5.0  \n",
              "1      SPELL      5.0  \n",
              "2         FN      5.0  \n",
              "3         FN      5.0  \n",
              "4        NaN      4.0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-362f3583-2e40-495f-8f23-76c72043ab91\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Label</th>\n",
              "      <th>Original Text</th>\n",
              "      <th>Corrected Text</th>\n",
              "      <th>Error Description</th>\n",
              "      <th>Error Type</th>\n",
              "      <th>Fluency</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>e</td>\n",
              "      <td>Σίγουρα η ραγδαία εξέλιξη των Επιστημών αλλάζο...</td>\n",
              "      <td>Σίγουρα η ραγδαία εξέλιξη των Επιστημών αλλάζε...</td>\n",
              "      <td>R</td>\n",
              "      <td>VERB:SVA</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>e</td>\n",
              "      <td>Αυτό αναπόφευκτα δημιουργεί νέα κοινωνικά και ...</td>\n",
              "      <td>Αυτό αναπόφευκτα δημιουργεί νέα κοινωνικά και ...</td>\n",
              "      <td>R</td>\n",
              "      <td>SPELL</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>e</td>\n",
              "      <td>Αυτό αναπόφευκτα δημιουργεί νέα κοινωνικά και ...</td>\n",
              "      <td>Αυτό αναπόφευκτα δημιουργεί νέα κοινωνικά και ...</td>\n",
              "      <td>M</td>\n",
              "      <td>FN</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>e</td>\n",
              "      <td>Αυτό αναπόφευκτα δημιουργεί νέα κοινωνικά και ...</td>\n",
              "      <td>Αυτό αναπόφευκτα δημιουργεί νέα κοινωνικά και ...</td>\n",
              "      <td>U</td>\n",
              "      <td>FN</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>c</td>\n",
              "      <td>Επομένως, οι επιφυλάξεις του σύγχρονου ανθρώπο...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-362f3583-2e40-495f-8f23-76c72043ab91')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-362f3583-2e40-495f-8f23-76c72043ab91 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-362f3583-2e40-495f-8f23-76c72043ab91');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('There are', len(ann.loc[(ann['Label'] == 'e')]), 'erroneous sentences')\n",
        "print('There are', len(ann.loc[(ann['Label'] == 'c')]), 'correct sentences')"
      ],
      "metadata": {
        "id": "Hf8Ze9gjTfuX",
        "outputId": "26d25ed1-63f2-4990-ab5a-3caa8c9feacd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 175 erroneous sentences\n",
            "There are 103 correct sentences\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# keep only incorrect sentences\n",
        "ann = ann.dropna(subset=['Error Type'])"
      ],
      "metadata": {
        "id": "Xi336FP5RfdA"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# API\n",
        "import errant\n",
        "import importlib\n",
        "importlib.reload(errant)\n",
        "\n",
        "annotator = errant.load('el')\n",
        "# parse sentences from dataframe.\n",
        "ann['orig'] = ann['Original Text'].apply(annotator.parse)\n",
        "ann['corr'] = ann['Corrected Text'].apply(annotator.parse)\n",
        "\n",
        "for index, row in ann.iterrows():  \n",
        "  edits = annotator.annotate(row['orig'], row['corr'])\n",
        "  for e in edits:    \n",
        "    print(e.o_start, e.o_end, e.o_str, e.c_start, e.c_end, e.c_str, e.type)"
      ],
      "metadata": {
        "id": "OLBvxwztP6ld"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}