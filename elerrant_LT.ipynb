{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "elerrant_LT.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "296ON-yTHgg8"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/katkorre/elerrant/blob/main/elerrant_LT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/katkorre/elerrant.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BgDcHOr6X0k1",
        "outputId": "cb66e339-e368-4105-e892-83c3efaa4888"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'elerrant'...\n",
            "remote: Enumerating objects: 204, done.\u001b[K\n",
            "remote: Counting objects: 100% (204/204), done.\u001b[K\n",
            "remote: Compressing objects: 100% (191/191), done.\u001b[K\n",
            "remote: Total 204 (delta 75), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (204/204), 3.90 MiB | 2.59 MiB/s, done.\n",
            "Resolving deltas: 100% (75/75), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EL.ERRANT (simply run this cell to use the tool)\n",
        "\n",
        "\n",
        "*   ERRANT is to automatically annotate parallel sentences with error type information.\n",
        "**Example** : **Original**: This are gramamtical sentence . **Corrected** : This is a grammatical sentence . ** Output M2** : `S This are gramamtical sentence.` \n",
        "`A 1 2|||R:VERB:SVA|||is|||REQUIRED|||-NONE-|||0 A 2 2|||M:DET|||a|||REQUIRED|||-NONE-|||0 A 2 3|||R:SPELL|||grammatical|||REQUIRED|||-NONE-|||0 A -1 -1|||noop|||-NONE-|||REQUIRED|||-NONE-|||1`\n",
        "\n"
      ],
      "metadata": {
        "id": "l2wvAPiFGVdJ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_MVSoFBBr4n3"
      },
      "source": [
        "# cloning original errant\n",
        "#! git clone https://github.com/chrisjbryant/errant.git --branch 2.3.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/chrisjbryant/errant.git --branch v2.3.0 --single-branch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QoWxlwLAxWY4",
        "outputId": "8d14f9c6-a47f-4f8d-fab3-f28969d22f17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'errant'...\n",
            "remote: Enumerating objects: 259, done.\u001b[K\n",
            "remote: Counting objects:   1% (1/70)\u001b[K\rremote: Counting objects:   2% (2/70)\u001b[K\rremote: Counting objects:   4% (3/70)\u001b[K\rremote: Counting objects:   5% (4/70)\u001b[K\rremote: Counting objects:   7% (5/70)\u001b[K\rremote: Counting objects:   8% (6/70)\u001b[K\rremote: Counting objects:  10% (7/70)\u001b[K\rremote: Counting objects:  11% (8/70)\u001b[K\rremote: Counting objects:  12% (9/70)\u001b[K\rremote: Counting objects:  14% (10/70)\u001b[K\rremote: Counting objects:  15% (11/70)\u001b[K\rremote: Counting objects:  17% (12/70)\u001b[K\rremote: Counting objects:  18% (13/70)\u001b[K\rremote: Counting objects:  20% (14/70)\u001b[K\rremote: Counting objects:  21% (15/70)\u001b[K\rremote: Counting objects:  22% (16/70)\u001b[K\rremote: Counting objects:  24% (17/70)\u001b[K\rremote: Counting objects:  25% (18/70)\u001b[K\rremote: Counting objects:  27% (19/70)\u001b[K\rremote: Counting objects:  28% (20/70)\u001b[K\rremote: Counting objects:  30% (21/70)\u001b[K\rremote: Counting objects:  31% (22/70)\u001b[K\rremote: Counting objects:  32% (23/70)\u001b[K\rremote: Counting objects:  34% (24/70)\u001b[K\rremote: Counting objects:  35% (25/70)\u001b[K\rremote: Counting objects:  37% (26/70)\u001b[K\rremote: Counting objects:  38% (27/70)\u001b[K\rremote: Counting objects:  40% (28/70)\u001b[K\rremote: Counting objects:  41% (29/70)\u001b[K\rremote: Counting objects:  42% (30/70)\u001b[K\rremote: Counting objects:  44% (31/70)\u001b[K\rremote: Counting objects:  45% (32/70)\u001b[K\rremote: Counting objects:  47% (33/70)\u001b[K\rremote: Counting objects:  48% (34/70)\u001b[K\rremote: Counting objects:  50% (35/70)\u001b[K\rremote: Counting objects:  51% (36/70)\u001b[K\rremote: Counting objects:  52% (37/70)\u001b[K\rremote: Counting objects:  54% (38/70)\u001b[K\rremote: Counting objects:  55% (39/70)\u001b[K\rremote: Counting objects:  57% (40/70)\u001b[K\rremote: Counting objects:  58% (41/70)\u001b[K\rremote: Counting objects:  60% (42/70)\u001b[K\rremote: Counting objects:  61% (43/70)\u001b[K\rremote: Counting objects:  62% (44/70)\u001b[K\rremote: Counting objects:  64% (45/70)\u001b[K\rremote: Counting objects:  65% (46/70)\u001b[K\rremote: Counting objects:  67% (47/70)\u001b[K\rremote: Counting objects:  68% (48/70)\u001b[K\rremote: Counting objects:  70% (49/70)\u001b[K\rremote: Counting objects:  71% (50/70)\u001b[K\rremote: Counting objects:  72% (51/70)\u001b[K\rremote: Counting objects:  74% (52/70)\u001b[K\rremote: Counting objects:  75% (53/70)\u001b[K\rremote: Counting objects:  77% (54/70)\u001b[K\rremote: Counting objects:  78% (55/70)\u001b[K\rremote: Counting objects:  80% (56/70)\u001b[K\rremote: Counting objects:  81% (57/70)\u001b[K\rremote: Counting objects:  82% (58/70)\u001b[K\rremote: Counting objects:  84% (59/70)\u001b[K\rremote: Counting objects:  85% (60/70)\u001b[K\rremote: Counting objects:  87% (61/70)\u001b[K\rremote: Counting objects:  88% (62/70)\u001b[K\rremote: Counting objects:  90% (63/70)\u001b[K\rremote: Counting objects:  91% (64/70)\u001b[K\rremote: Counting objects:  92% (65/70)\u001b[K\rremote: Counting objects:  94% (66/70)\u001b[K\rremote: Counting objects:  95% (67/70)\u001b[K\rremote: Counting objects:  97% (68/70)\u001b[K\rremote: Counting objects:  98% (69/70)\u001b[K\rremote: Counting objects: 100% (70/70)\u001b[K\rremote: Counting objects: 100% (70/70), done.\u001b[K\n",
            "remote: Compressing objects: 100% (36/36), done.\u001b[K\n",
            "remote: Total 259 (delta 34), reused 69 (delta 34), pack-reused 189\u001b[K\n",
            "Receiving objects: 100% (259/259), 1.45 MiB | 10.28 MiB/s, done.\n",
            "Resolving deltas: 100% (132/132), done.\n",
            "Note: checking out '9111c6c5ca0dffdd5d8023faab91cc94aa3aef93'.\n",
            "\n",
            "You are in 'detached HEAD' state. You can look around, make experimental\n",
            "changes and commit them, and you can discard any commits you make in this\n",
            "state without impacting any branches by performing another checkout.\n",
            "\n",
            "If you want to create a new branch to retain commits you create, you may\n",
            "do so (now or later) by using -b with the checkout command again. Example:\n",
            "\n",
            "  git checkout -b <new-branch-name>\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9uFPgFJsx92",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80e8aba3-f63f-4057-e699-e380c1563f5b"
      },
      "source": [
        "# download GREEK hunspell dict\n",
        "! npx degit katkorre/elerrant/resources/el_GR.txt el_GR.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K\u001b[?25hnpx: installed 1 in 3.492s\n",
            "\u001b[36m> cloned \u001b[1mkatkorre/elerrant\u001b[22m#\u001b[1mHEAD\u001b[22m to el_GR.txt\u001b[39m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7sTuuLmtGN7"
      },
      "source": [
        "# make directory for elerrant resources and move hunspell dict in\n",
        "!mkdir errant/errant/el \n",
        "!mkdir errant/errant/el/resources\n",
        "!mv /content/el_GR.txt errant/errant/el/resources/el_GR.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCb0oxErucZb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50db3991-7a88-4197-f403-717be5872532"
      },
      "source": [
        "# install greek-stemmer, greek spacy, and levenshtein distance\n",
        "! pip install greek-stemmer\n",
        "! python -m spacy download el\n",
        "! pip install python-Levenshtein\n",
        "#!pip install rapidfuzz\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting greek-stemmer\n",
            "  Downloading greek_stemmer-0.1.1.tar.gz (6.9 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from greek-stemmer) (3.13)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from greek-stemmer) (0.16.0)\n",
            "Building wheels for collected packages: greek-stemmer\n",
            "  Building wheel for greek-stemmer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for greek-stemmer: filename=greek_stemmer-0.1.1-py3-none-any.whl size=6738 sha256=e29e8b756ed94e8a23914314b120c8f070765a397c9a51d77fb3dbf12707d95e\n",
            "  Stored in directory: /root/.cache/pip/wheels/c5/39/16/eec6bf21a071c9e3ff11cda038b91d33fcce20a0b7378f64ac\n",
            "Successfully built greek-stemmer\n",
            "Installing collected packages: greek-stemmer\n",
            "Successfully installed greek-stemmer-0.1.1\n",
            "Collecting el_core_news_sm==2.2.5\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/el_core_news_sm-2.2.5/el_core_news_sm-2.2.5.tar.gz (11.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 11.4 MB 22.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from el_core_news_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->el_core_news_sm==2.2.5) (2.0.6)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->el_core_news_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->el_core_news_sm==2.2.5) (1.0.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->el_core_news_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->el_core_news_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->el_core_news_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->el_core_news_sm==2.2.5) (0.9.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->el_core_news_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->el_core_news_sm==2.2.5) (1.21.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->el_core_news_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->el_core_news_sm==2.2.5) (57.4.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->el_core_news_sm==2.2.5) (3.0.6)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->el_core_news_sm==2.2.5) (4.63.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->el_core_news_sm==2.2.5) (4.11.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->el_core_news_sm==2.2.5) (3.10.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->el_core_news_sm==2.2.5) (3.7.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->el_core_news_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->el_core_news_sm==2.2.5) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->el_core_news_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->el_core_news_sm==2.2.5) (1.24.3)\n",
            "Building wheels for collected packages: el-core-news-sm\n",
            "  Building wheel for el-core-news-sm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for el-core-news-sm: filename=el_core_news_sm-2.2.5-py3-none-any.whl size=11422784 sha256=3d0a552972525f81f57e3850437fb953b6a4762e5fcc4ac9b035f8b55d089695\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-hsror705/wheels/42/c5/56/c856612ccb301751cd0a8e140b7714b68ec111699b8feea01a\n",
            "Successfully built el-core-news-sm\n",
            "Installing collected packages: el-core-news-sm\n",
            "Successfully installed el-core-news-sm-2.2.5\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('el_core_news_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/el_core_news_sm -->\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/data/el\n",
            "You can now load the model via spacy.load('el')\n",
            "Collecting python-Levenshtein\n",
            "  Downloading python-Levenshtein-0.12.2.tar.gz (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 5.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from python-Levenshtein) (57.4.0)\n",
            "Building wheels for collected packages: python-Levenshtein\n",
            "  Building wheel for python-Levenshtein (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-Levenshtein: filename=python_Levenshtein-0.12.2-cp37-cp37m-linux_x86_64.whl size=149861 sha256=1ff3b7d03e512046d00070d4d437f32d1e39b6377bc83827a74a55db1e7c29b2\n",
            "  Stored in directory: /root/.cache/pip/wheels/05/5f/ca/7c4367734892581bb5ff896f15027a932c551080b2abd3e00d\n",
            "Successfully built python-Levenshtein\n",
            "Installing collected packages: python-Levenshtein\n",
            "Successfully installed python-Levenshtein-0.12.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajDBB2m0vHX-"
      },
      "source": [
        "!cp errant/errant/en/merger.py errant/errant/el/\n",
        "!cp -r errant/errant ./_errant\n",
        "!rm -r errant\n",
        "!mv _errant errant"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ufTwdQOlvMu8",
        "outputId": "17a8976a-1a2f-49f1-a6f6-b7b2528d9168"
      },
      "source": [
        "%%writefile errant/__init__.py\n",
        "import Levenshtein\n",
        "from importlib import import_module\n",
        "import spacy\n",
        "from errant.annotator import Annotator\n",
        "\n",
        "\n",
        "# ERRANT version\n",
        "__version__ = '2.2.3'\n",
        "\n",
        "# Load an ERRANT Annotator object for a given language\n",
        "def load(lang, nlp=None):\n",
        "    # Make sure the language is supported\n",
        "    supported = {\"en\", \"el\"}\n",
        "    if lang not in supported:\n",
        "        raise Exception(\"%s is an unsupported or unknown language\" % lang)\n",
        "\n",
        "    # Load spacy\n",
        "    nlp = nlp or spacy.load(lang, disable=[\"ner\"])\n",
        "\n",
        "    # Load language edit merger\n",
        "    merger = import_module(\"errant.%s.merger\" % lang)\n",
        "\n",
        "    # Load language edit classifier\n",
        "    classifier = import_module(\"errant.%s.classifier\" % lang)\n",
        "    # The English classifier needs spacy\n",
        "    # Also adding the Greek classifier\n",
        "    if lang in {\"en\", \"el\"}: classifier.nlp = nlp\n",
        "\n",
        "    # Return a configured ERRANT annotator\n",
        "    return Annotator(lang, nlp, merger, classifier)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting errant/__init__.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JX-PE_30veF8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87123680-9cc0-48f2-e550-4e3a0486a8dc"
      },
      "source": [
        "%%writefile errant/el/classifier.py\n",
        "import re\n",
        "from pathlib import Path\n",
        "#from rapidfuzz.distance import Levenshtein\n",
        "import Levenshtein\n",
        "#use greek stemmer https://pypi.org/project/greek-stemmer/\n",
        "from greek_stemmer import GreekStemmer\n",
        "import spacy\n",
        "import spacy.symbols as POS\n",
        "\n",
        "# Load Greek Hunspell word list\n",
        "def load_word_list(path):\n",
        "    with open(path) as word_list:\n",
        "        return set([word.strip() for word in word_list])\n",
        "\n",
        "\n",
        "\n",
        "# Classifier resources\n",
        "base_dir = \"errant/errant/en\"\n",
        "# Spacy\n",
        "nlp = None\n",
        "# Greek Stemmer\n",
        "stemmer = GreekStemmer()\n",
        "# Greek Word list\n",
        "spell = load_word_list('/content/errant/el/resources/el_GR.txt')\n",
        "\n",
        "# Rare POS tags that make uninformative error categories\n",
        "rare_pos = {\"INTJ\", \"NUM\", \"SYM\", \"X\"}\n",
        "# Open class coarse Spacy POS tags \n",
        "open_pos1 = {POS.ADJ, POS.ADV, POS.NOUN, POS.VERB}\n",
        "# Open class coarse Spacy POS tags (strings)\n",
        "open_pos2 = {\"ADJ\", \"ADV\", \"NOUN\", \"VERB\"}\n",
        "# POS tags with inflectional morphology\n",
        "inflected_tags = {\"ADJ\", \"ADV\", \"AUX\", \"DET\", \"PRON\", \"PROPN\", \"NOUN\", \"VERB\"}\n",
        "# Some dep labels that map to pos tags.\n",
        "dep_map = {\"ac\": \"ADP\", \"svp\": \"ADP\",\t\"punct\": \"PUNCT\", \"CCONJ\": \"CONJ\" }\n",
        "# Accents/Vowels\n",
        "accents=['ά','έ', 'ή', 'ί', 'ό', 'ύ', 'ώ']\n",
        "#Simplified cats\n",
        "simple_cats={'CCONJ':'CONJ', 'SCONJ':'CONJ', 'ADP':'PREP' }\n",
        "\n",
        "\n",
        "# Input: An Edit object\n",
        "# Output: The same Edit object with an updated error type\n",
        "def classify(edit):  \n",
        "    # Nothing to nothing is a detected but not corrected edit\n",
        "    if not edit.o_toks and not edit.c_toks:\n",
        "        edit.type = \"UNK\"\n",
        "    # Missing\n",
        "    elif not edit.o_toks and edit.c_toks:\n",
        "        op = \"M:\"\n",
        "        cat = simplify(get_one_sided_type(edit.c_toks))\n",
        "        edit.type = op+cat   \n",
        "    # Unnecessary\n",
        "    elif edit.o_toks and not edit.c_toks:\n",
        "        op = \"U:\"\n",
        "        cat = simplify(get_one_sided_type(edit.o_toks))\n",
        "        edit.type = op+cat\n",
        "    # Replacement and special cases\n",
        "    else:\n",
        "        # Same to same is a detected but not corrected edit\n",
        "        if edit.o_str == edit.c_str:\n",
        "            edit.type = \"UNK\"\n",
        "        # Classify the edit as if the last token wasn't there\n",
        "        elif edit.o_toks[-1].lower == edit.c_toks[-1].lower and \\\n",
        "                (len(edit.o_toks) > 1 or len(edit.c_toks) > 1):\n",
        "            # Store a copy of the full orig and cor toks\n",
        "            all_o_toks = edit.o_toks[:]\n",
        "            all_c_toks = edit.c_toks[:]\n",
        "            # Truncate the instance toks for classification\n",
        "            edit.o_toks = edit.o_toks[:-1]\n",
        "            edit.c_toks = edit.c_toks[:-1]\n",
        "            # Classify the truncated edit\n",
        "            edit = classify(edit)\n",
        "            # Restore the full orig and cor toks\n",
        "            edit.o_toks = all_o_toks\n",
        "            edit.c_toks = all_c_toks\n",
        "\n",
        "        # Accent/Final Nu special cases\n",
        "        #these need to go to replacement\n",
        "        elif accent(edit.o_toks, edit.c_toks) == \"miss_acc\":\n",
        "          edit.type = \"M:ACC\"\n",
        "        elif accent(edit.o_toks, edit.c_toks) == \"unn_acc\":\n",
        "          edit.type = \"U:ACC\"\n",
        "        elif final_n(edit.o_toks, edit.c_toks) == 'unn_fn':\n",
        "          edit.type = \"U:FN\"\n",
        "        elif final_n(edit.o_toks, edit.c_toks) == 'miss_fn':\n",
        "          edit.type = \"M:FN\"\n",
        "        # Replacement\n",
        "        else:\n",
        "            op = \"R:\"\n",
        "            cat = simplify(get_two_sided_type(edit.o_toks, edit.c_toks))\n",
        "            edit.type = op+cat\n",
        "    return edit\n",
        "\n",
        "\n",
        "\n",
        "# Input: Spacy tokens\n",
        "# Output: A list of pos and dep tag strings\n",
        "def get_edit_info(toks):\n",
        "    pos = []\n",
        "    dep = []\n",
        "    for tok in toks:\n",
        "        pos.append(tok.tag_)\n",
        "        dep.append(tok.dep_)\n",
        "    return pos, dep\n",
        "\n",
        "# Input: Spacy tokens\n",
        "# Output: An error type string based on input tokens from orig or cor\n",
        "# When one side of the edit is null, we can only use the other side\n",
        "def get_one_sided_type(toks):\n",
        "    # Special cases\n",
        "    if len(toks) == 1:\n",
        "        # Subjunctive \"να\" is treated as part of a verb form\n",
        "        if toks[0].lower_ == \"να\" and toks[0].pos == POS.PART :\n",
        "            return \"VERB:FORM\"     \n",
        "        \n",
        "    # Extract pos tags and parse info from the toks\n",
        "    pos_list, dep_list = get_edit_info(toks)\n",
        "    # Auxiliary verbs e.g \"έχω, είχα\" \n",
        "        # Μέλλοντας \"θα\"\n",
        "    if toks[0].lower_ == \"θα\" and toks[0].pos == POS.PART :\n",
        "        return \"VERB:TENSE\"\n",
        "    if toks[0].pos == POS.VERB and set(dep_list).issubset({\"aux\", \"auxpass\", \"obj\", \"advmod\"}):\n",
        "        return \"VERB:TENSE\"\n",
        "    # POS-based tags. Ignores rare, uninformative categories\n",
        "    if len(set(pos_list)) == 1 and pos_list[0] not in rare_pos:\n",
        "        return pos_list[0]\n",
        "    # More POS-based tags using special dependency labels\n",
        "    if len(set(dep_list)) == 1 and dep_list[0] in dep_map.keys():\n",
        "        return dep_map[dep_list[0]]\n",
        "    # Tricky cases\n",
        "    else:\n",
        "        return \"OTHER\"\n",
        "\n",
        "\n",
        "# Input 1: Spacy orig tokens\n",
        "# Input 2: Spacy cor tokens\n",
        "# Output: An error type string based on orig AND cor\n",
        "def get_two_sided_type(o_toks, c_toks):\n",
        "    # Extract pos tags and parse info from the toks as lists\n",
        "    o_pos, o_dep = get_edit_info(o_toks)\n",
        "    c_pos, c_dep = get_edit_info(c_toks)\n",
        "\n",
        "    # Orthography; i.e. whitespace and/or case errors.\n",
        "    if only_orth_change(o_toks, c_toks):\n",
        "        return \"ORTH\"\n",
        "    # Word Order; only matches exact reordering.\n",
        "    if exact_reordering(o_toks, c_toks):\n",
        "        return \"WO\"   \n",
        "             \n",
        "\n",
        "# 2. SPELLING AND INFLECTION\n",
        "        # Only check alphabetical strings on the original side\n",
        "        # Spelling errors take precedence over POS errors; this rule is ordered\n",
        "        if o_toks[0].text.isalpha():          \n",
        "            # Check a greek dict for both orig and lower case.            \n",
        "            if o_toks[0].text not in spell and \\\n",
        "                    o_toks[0].lower_ not in spell:\n",
        "                # Check if both sides have a common lemma\n",
        "                if o_toks[0].lemma == c_toks[0].lemma:\n",
        "                    # Inflection\n",
        "                    # Spacy issue returns nonetype when does not properly assign pos\n",
        "                    if o_pos == c_pos and o_pos[0] in {\"NOUN\", \"ADJ\", \"ADV\", \"PRON\", \"VERB\"}:\n",
        "                        return o_pos[0]+\":FORM\"\n",
        "                    # Unknown morphology; i.e. we cannot be more specific.\n",
        "                    else:\n",
        "                        return \"MORPH\"\n",
        "                # Use string similarity to detect true spelling errors.\n",
        "                else:\n",
        "                  char_ratio = Levenshtein.ratio(o_toks[0].text, c_toks[0].text)\n",
        "                  # Ratio > 0.5 means both side share at least half the same chars.\n",
        "                  # WARNING: THIS IS AN APPROXIMATION.\n",
        "                  if char_ratio > 0.5:\n",
        "                      return \"SPELL\"\n",
        "                  # If ratio is <= 0.5, the error is more complex\n",
        "                  else:\n",
        "                      # If POS is the same, this takes precedence over spelling.\n",
        "                      if o_pos == c_pos and \\\n",
        "                              o_pos[0] not in rare_pos:\n",
        "                          return o_pos[0]\n",
        "                      # Tricky cases.\n",
        "                      \n",
        "                      if char_ratio > 0.9:\n",
        "                        if o_toks[0] not in conts and c_toks[0] not in conts and \\\n",
        "                        accent(o_toks,c_toks) == \"repl_acc\":\n",
        "                          return \"ACC\"\n",
        "                      else:\n",
        "                        return \"OTHER\"   \n",
        "\n",
        "        # 3. MORPHOLOGY\n",
        "        # Only ADJ, ADV, NOUN and VERB can have inflectional changes.\n",
        "        if o_toks[0].lemma == c_toks[0].lemma and \\\n",
        "                o_pos[0] in open_pos2 and \\\n",
        "                c_pos[0] in open_pos2:\n",
        "            # Same POS on both sides\n",
        "            if o_pos == c_pos:\n",
        "                # Adjective form; e.g. comparatives\n",
        "                if o_pos[0] == \"ADJ\":\n",
        "                    return \"ADJ:FORM\"\n",
        "                # Noun number\n",
        "                if o_pos[0] == \"NOUN\":\n",
        "                    return \"NOUN:FORM\"\n",
        "                # Verbs - various types\n",
        "                if o_pos[0] == \"VERB\":\n",
        "                    # NOTE: These rules are carefully ordered.\n",
        "                    # Use the dep parse to find some form errors.\n",
        "                    # Main verbs preceded by aux cannot be tense or SVA.\n",
        "                    if preceded_by_aux(o_toks, c_toks):\n",
        "                        return \"VERB:FORM\"\n",
        "                if o_pos == c_pos and o_pos[0] == \"VERB\":\n",
        "                        return \"VERB:FORM\"\n",
        "                        \n",
        "\n",
        "        # 4. GENERAL\n",
        "        # Auxiliaries with different lemmas\n",
        "        if o_dep[0].startswith(\"aux\") and c_dep[0].startswith(\"aux\"):\n",
        "            return \"VERB:TENSE\"\n",
        "        # POS-based tags. Some of these are context sensitive mispellings.\n",
        "        if o_pos == c_pos and o_pos[0] not in rare_pos:\n",
        "            return o_pos[0]\n",
        "        # Some dep labels map to POS-based tags.\n",
        "        if o_dep == c_dep and o_dep[0] in dep_map.keys():\n",
        "            return dep_map[o_dep[0]]\n",
        "        else:\n",
        "            return \"OTHER\"\n",
        "\n",
        "    # Multi-token replacements (uncommon)\n",
        "    # All auxiliaries\n",
        "    if set(o_dep+c_dep).issubset({\"aux\", \"auxpass\"}):\n",
        "        return \"VERB:TENSE\"\n",
        "    # All same POS\n",
        "    if len(set(o_pos+c_pos)) == 1:\n",
        "        # Final verbs with the same lemma are tense\n",
        "        if o_pos[0] == \"VERB\" and \\\n",
        "                o_toks[-1].lemma == c_toks[-1].lemma:\n",
        "            return \"VERB:TENSE\"\n",
        "        # POS-based tags.\n",
        "        elif o_pos[0] not in rare_pos:\n",
        "            return o_pos[0]\n",
        "    # All same special dep labels.\n",
        "    if len(set(o_dep+c_dep)) == 1 and \\\n",
        "            o_dep[0] in dep_map.keys():\n",
        "        return dep_map[o_dep[0]]\n",
        "    # Verbs with particles e.g., μην κάνεις\n",
        "    if set(o_pos+c_pos) == {\"PART\", \"VERB\"}:\n",
        "      \n",
        "        if o_toks[-1].lemma == c_toks[-1].lemma:\n",
        "            return \"VERB:FORM\"\n",
        "        # In case particle needs to go\n",
        "        else:\n",
        "            return \"VERB\"\n",
        "    # Tricky cases.\n",
        "    else:\n",
        "        return \"OTHER\"\n",
        "\n",
        "\n",
        "def only_orth_change(o_toks, c_toks):\n",
        "    o_join = \"\".join([o.lower_ for o in o_toks])\n",
        "    c_join = \"\".join([c.lower_ for c in c_toks])\n",
        "    if o_join == c_join:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "# Input 1: Spacy orig tokens\n",
        "# Input 2: Spacy cor tokens\n",
        "# Output: Boolean; the tokens are exactly the same but in a different order\n",
        "def exact_reordering(o_toks, c_toks):\n",
        "    # Sorting lets us keep duplicates.\n",
        "    o_set = sorted([o.lower_ for o in o_toks])\n",
        "    c_set = sorted([c.lower_ for c in c_toks])\n",
        "    if o_set == c_set:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "\n",
        "\n",
        "def accent(o_toks, c_toks):\n",
        "  o_toks = str(o_toks)\n",
        "  c_toks = str(c_toks)\n",
        "  o_chars =[char for char in o_toks]\n",
        "  c_chars = [char for char in c_toks]\n",
        "   \n",
        "  char_list1=[]  \n",
        "  char_list2=[]  \n",
        "\n",
        "  if set(o_chars).isdisjoint(accents) == False and set(c_chars).isdisjoint(accents) == True:\n",
        "    return \"unn_acc\"\n",
        "  elif set(o_chars).isdisjoint(accents) == True and set(c_chars).isdisjoint(accents) == False:\n",
        "   return \"miss_acc\"\n",
        "  elif set(o_chars).isdisjoint(accents) == False and set(c_chars).isdisjoint(accents) == False:\n",
        "    char1 = list(set(o_chars).intersection(accents))\n",
        "    char2 = list(set(c_chars).intersection(accents))\n",
        "    if len(char2)>1:\n",
        "      return 'miss_acc'\n",
        "    elif len(char1)>1:\n",
        "      return 'unn_acc'\n",
        "    else:\n",
        "      len(char1) == len(char2)\n",
        "      if o_chars.index(char1[0]) != c_chars.index(char2[0]): \n",
        "        return'repl_acc'\n",
        "\n",
        "    \n",
        "def final_n(o_toks, c_toks):\n",
        "  o_toks = str(o_toks)\n",
        "  c_toks = str(c_toks)\n",
        "  o_chars =[char for char in o_toks]\n",
        "  c_chars = [char for char in c_toks]\n",
        "\n",
        "  if o_chars == c_chars[:-1] and c_chars[-1]=='ν':\n",
        "    return \"miss_fn\"\n",
        "  elif o_chars[:-1] == c_chars and o_chars[-1]=='ν':\n",
        "    return \"unn_fn\"\n",
        "\n",
        "def simplify(cat):\n",
        "  if cat in simple_cats:\n",
        "    cat = simple_cats.get(cat)\n",
        "  return cat"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing errant/el/classifier.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YhwkEcgf0FaW"
      },
      "source": [
        "import errant\n",
        "import argparse\n",
        "from contextlib import ExitStack\n",
        "import errant\n",
        "\n",
        "def parallel(out, orig, cor, tok = False, merge ='rules', lev = False ):\n",
        "  print(\"Loading resources...\")\n",
        "  # Load Errant\n",
        "  annotator = errant.load(\"en\")\n",
        "  # Open output m2 file\n",
        "  out_m2 = open(out, \"w\")\n",
        "\n",
        "  print(\"Processing parallel files...\")\n",
        "  # Process an arbitrary number of files line by line simultaneously. Python 3.3+\n",
        "  # See https://tinyurl.com/y4cj4gth\n",
        "  with ExitStack() as stack:\n",
        "      in_files = [stack.enter_context(open(i)) for i in [orig]+cor]\n",
        "      # Process each line of all input files\n",
        "      for line in zip(*in_files):\n",
        "          # Get the original and all the corrected texts\n",
        "          orig = line[0].strip()\n",
        "          cors = line[1:]\n",
        "          # Skip the line if orig is empty\n",
        "          if not orig: continue\n",
        "          # Parse orig with spacy\n",
        "          orig = annotator.parse(orig, tok)\n",
        "          # Write orig to the output m2 file\n",
        "          out_m2.write(\" \".join([\"S\"]+[token.text for token in orig])+\"\\n\")\n",
        "          # Loop through the corrected texts\n",
        "          for cor_id, cor in enumerate(cors):\n",
        "              cor = cor.strip()\n",
        "              # If the texts are the same, write a noop edit\n",
        "              if orig.text.strip() == cor:\n",
        "                  out_m2.write(noop_edit(cor_id)+\"\\n\")\n",
        "              # Otherwise, do extra processing\n",
        "              else:\n",
        "                  # Parse cor with spacy\n",
        "                  cor = annotator.parse(cor, tok)\n",
        "                  # Align the texts and extract and classify the edits\n",
        "                  edits = annotator.annotate(orig, cor, lev, merge)\n",
        "                  # Loop through the edits\n",
        "                  for edit in edits:\n",
        "                      # Write the edit to the output m2 file\n",
        "                      out_m2.write(edit.to_m2(cor_id)+\"\\n\")\n",
        "          # Write a newline when we have processed all corrections for each line\n",
        "          out_m2.write(\"\\n\")\n",
        "          \n",
        "# Input: A coder id\n",
        "# Output: A noop edit; i.e. text contains no edits\n",
        "def noop_edit(id=0):\n",
        "    return \"A -1 -1|||noop|||-NONE-|||REQUIRED|||-NONE-|||\"+str(id)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create text files for annotation\n",
        "\n",
        "*   There must be a file for the original sentences and a separate file for the corrections.\n",
        "\n"
      ],
      "metadata": {
        "id": "296ON-yTHgg8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The original\n",
        "with open(\"orig.txt\", \"w\") as o:\n",
        "  o.write(\"Γεια σου\")\n",
        "\n",
        "# The corrected\n",
        "with open(\"corr.txt\", \"w\") as o:\n",
        "  o.write(\"Γεια\")"
      ],
      "metadata": {
        "id": "-wRW2M789TRx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Then employ `elerrant` to read the found errors:"
      ],
      "metadata": {
        "id": "fWhkj3rs-KUp"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bvMJ2apNVEj"
      },
      "source": [
        "%%capture\n",
        "parallel('out_m2','orig.txt',['corr.txt'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Flag explanations:\n",
        "\n",
        "`(1)A 1 2|||(2)R:VERB|||(3)είναι|||(4)REQUIRED|||(5)-NONE-|||(6)0' \n",
        "\n",
        "1. Error spans (which tokens convey the error)\n",
        "2. Error type\n",
        "3. Correction\n",
        "4. Required or not required field(not particular use)\n",
        "5. -NONE- is also a not interpretable flag\n",
        "6. Annotators id \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KI3FnBbW-Ux8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "open(\"out_m2\").read()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yI2fvNjS9tzy",
        "outputId": "9fac3742-f00e-45ca-b8c4-95096aa4722f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'S Γεια σου\\nA 1 2|||U:NOUN||||||REQUIRED|||-NONE-|||0\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GNC example"
      ],
      "metadata": {
        "id": "iQke6FpVYN3i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "parallel('out_m2','/content/elerrant/GNC/orig.txt',['/content/elerrant/GNC/corr.txt'])"
      ],
      "metadata": {
        "id": "LR_bzCx_bDun"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scorer"
      ],
      "metadata": {
        "id": "h6pHCniCRPQv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python errant/commands/compare_m2.py -hyp out_m2 -ref out2_m2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ArRaIybfRRvJ",
        "outputId": "85bc0f2c-7c3a-4524-b32c-958d6447b7c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"errant/commands/compare_m2.py\", line 381, in <module>\n",
            "    main()\n",
            "  File \"errant/commands/compare_m2.py\", line 9, in main\n",
            "    ref_m2 = open(args.ref).read().strip().split(\"\\n\\n\")\n",
            "FileNotFoundError: [Errno 2] No such file or directory: 'out2_m2'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python errant/commands/compare_m2.py -hyp out_m2 -ref out2_m2 -cat {2}"
      ],
      "metadata": {
        "id": "MNPmPNtJll_q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b441827d-8587-455a-95cf-5d39b081daef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"errant/commands/compare_m2.py\", line 381, in <module>\n",
            "    main()\n",
            "  File \"errant/commands/compare_m2.py\", line 9, in main\n",
            "    ref_m2 = open(args.ref).read().strip().split(\"\\n\\n\")\n",
            "FileNotFoundError: [Errno 2] No such file or directory: 'out2_m2'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python errant/commands/compare_m2.py -hyp out_m2 -ref out2_m2 -ds"
      ],
      "metadata": {
        "id": "KwJsO2gNloFi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78c8b462-a024-410e-9726-c5257143d6ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"errant/commands/compare_m2.py\", line 381, in <module>\n",
            "    main()\n",
            "  File \"errant/commands/compare_m2.py\", line 9, in main\n",
            "    ref_m2 = open(args.ref).read().strip().split(\"\\n\\n\")\n",
            "FileNotFoundError: [Errno 2] No such file or directory: 'out2_m2'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python errant/commands/compare_m2.py -hyp out_m2 -ref out2_m2 -ds -cat {1}"
      ],
      "metadata": {
        "id": "eQWrKGvRlpe6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3cf021b0-2ae5-478e-ab57-03849b9c0190"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"errant/commands/compare_m2.py\", line 381, in <module>\n",
            "    main()\n",
            "  File \"errant/commands/compare_m2.py\", line 9, in main\n",
            "    ref_m2 = open(args.ref).read().strip().split(\"\\n\\n\")\n",
            "FileNotFoundError: [Errno 2] No such file or directory: 'out2_m2'\n"
          ]
        }
      ]
    }
  ]
}