{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "elerrant.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN6PIeostpAGHlrKcwVS1wX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/katkorre/elerrant/blob/main/elerrant.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_MVSoFBBr4n3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36e86b3c-6c29-44fc-85a6-61c6d7e8c0e3"
      },
      "source": [
        "# cloning original errant\n",
        "! git clone https://github.com/chrisjbryant/errant.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'errant'...\n",
            "remote: Enumerating objects: 271, done.\u001b[K\n",
            "remote: Counting objects: 100% (81/81), done.\u001b[K\n",
            "remote: Compressing objects: 100% (46/46), done.\u001b[K\n",
            "remote: Total 271 (delta 44), reused 69 (delta 35), pack-reused 190\u001b[K\n",
            "Receiving objects: 100% (271/271), 1.45 MiB | 11.40 MiB/s, done.\n",
            "Resolving deltas: 100% (143/143), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9uFPgFJsx92",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "490b0075-0569-46fb-f4e7-a966026ea94d"
      },
      "source": [
        "# download GREEK hunspell dict\n",
        "! npx degit katkorre/elerrant/resources/el_GR.txt el_GR.txt"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K\u001b[?25hnpx: installed 1 in 3.51s\n",
            "\u001b[36m> cloned \u001b[1mkatkorre/elerrant\u001b[22m#\u001b[1mHEAD\u001b[22m to el_GR.txt\u001b[39m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7sTuuLmtGN7"
      },
      "source": [
        "# make directory for elerrant resources and move hunspell dict in\n",
        "!mkdir errant/errant/el \n",
        "!mkdir errant/errant/el/resources\n",
        "!mv /content/el_GR.txt errant/errant/el/resources/el_GR.txt"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCb0oxErucZb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6c8a781-8313-4b9d-b67c-5822a8a2a588"
      },
      "source": [
        "# install greek-stemmer, greek spacy, and levenshtein distance\n",
        "! pip install greek-stemmer\n",
        "! python -m spacy download el\n",
        "! pip install python-Levenshtein"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting greek-stemmer\n",
            "  Downloading greek_stemmer-0.1.1.tar.gz (6.9 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from greek-stemmer) (3.13)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from greek-stemmer) (0.16.0)\n",
            "Building wheels for collected packages: greek-stemmer\n",
            "  Building wheel for greek-stemmer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for greek-stemmer: filename=greek_stemmer-0.1.1-py3-none-any.whl size=6737 sha256=112de56b2ee1615666c73483ca8652d5aec9c09b56d8d1d9d9c2b51e9b86002d\n",
            "  Stored in directory: /root/.cache/pip/wheels/c5/39/16/eec6bf21a071c9e3ff11cda038b91d33fcce20a0b7378f64ac\n",
            "Successfully built greek-stemmer\n",
            "Installing collected packages: greek-stemmer\n",
            "Successfully installed greek-stemmer-0.1.1\n",
            "Collecting el_core_news_sm==2.2.5\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/el_core_news_sm-2.2.5/el_core_news_sm-2.2.5.tar.gz (11.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 11.4 MB 2.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from el_core_news_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->el_core_news_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->el_core_news_sm==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->el_core_news_sm==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->el_core_news_sm==2.2.5) (4.62.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->el_core_news_sm==2.2.5) (57.4.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->el_core_news_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->el_core_news_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->el_core_news_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->el_core_news_sm==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->el_core_news_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->el_core_news_sm==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->el_core_news_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->el_core_news_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->el_core_news_sm==2.2.5) (4.6.4)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->el_core_news_sm==2.2.5) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->el_core_news_sm==2.2.5) (3.5.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->el_core_news_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->el_core_news_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->el_core_news_sm==2.2.5) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->el_core_news_sm==2.2.5) (1.24.3)\n",
            "Building wheels for collected packages: el-core-news-sm\n",
            "  Building wheel for el-core-news-sm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for el-core-news-sm: filename=el_core_news_sm-2.2.5-py3-none-any.whl size=11422784 sha256=91b19e6b8c3888834f53a10874f5764f1396848a3bf93cac2ac0e9977b0d1b7d\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-gwhad75b/wheels/42/c5/56/c856612ccb301751cd0a8e140b7714b68ec111699b8feea01a\n",
            "Successfully built el-core-news-sm\n",
            "Installing collected packages: el-core-news-sm\n",
            "Successfully installed el-core-news-sm-2.2.5\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('el_core_news_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/el_core_news_sm -->\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/data/el\n",
            "You can now load the model via spacy.load('el')\n",
            "Collecting python-Levenshtein\n",
            "  Downloading python-Levenshtein-0.12.2.tar.gz (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 2.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from python-Levenshtein) (57.4.0)\n",
            "Building wheels for collected packages: python-Levenshtein\n",
            "  Building wheel for python-Levenshtein (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-Levenshtein: filename=python_Levenshtein-0.12.2-cp37-cp37m-linux_x86_64.whl size=149861 sha256=77dfd1f94cca2614e7bb605adcc13d2205feae071ae73c9d3a1b69bc6141a523\n",
            "  Stored in directory: /root/.cache/pip/wheels/05/5f/ca/7c4367734892581bb5ff896f15027a932c551080b2abd3e00d\n",
            "Successfully built python-Levenshtein\n",
            "Installing collected packages: python-Levenshtein\n",
            "Successfully installed python-Levenshtein-0.12.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajDBB2m0vHX-"
      },
      "source": [
        "!cp errant/errant/en/merger.py errant/errant/el/\n",
        "!cp -r errant/errant ./_errant\n",
        "!rm -r errant\n",
        "!mv _errant errant"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ufTwdQOlvMu8",
        "outputId": "64cd9daf-06ce-4314-a77b-b19172f0605b"
      },
      "source": [
        "%%writefile errant/__init__.py\n",
        "\n",
        "from importlib import import_module\n",
        "import spacy\n",
        "from errant.annotator import Annotator\n",
        "\n",
        "\n",
        "# ERRANT version\n",
        "__version__ = '2.2.3'\n",
        "\n",
        "# Load an ERRANT Annotator object for a given language\n",
        "def load(lang, nlp=None):\n",
        "    # Make sure the language is supported\n",
        "    supported = {\"en\", \"el\"}\n",
        "    if lang not in supported:\n",
        "        raise Exception(\"%s is an unsupported or unknown language\" % lang)\n",
        "\n",
        "    # Load spacy\n",
        "    nlp = nlp or spacy.load(lang, disable=[\"ner\"])\n",
        "\n",
        "    # Load language edit merger\n",
        "    merger = import_module(\"errant.%s.merger\" % lang)\n",
        "\n",
        "    # Load language edit classifier\n",
        "    classifier = import_module(\"errant.%s.classifier\" % lang)\n",
        "    # The English classifier needs spacy\n",
        "    # Also adding the Greek classifier\n",
        "    if lang in {\"en\", \"el\"}: classifier.nlp = nlp\n",
        "\n",
        "    # Return a configured ERRANT annotator\n",
        "    return Annotator(lang, nlp, merger, classifier)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting errant/__init__.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JX-PE_30veF8",
        "outputId": "440fdeeb-fc56-481b-d375-7826a9856ea0"
      },
      "source": [
        "%%writefile errant/el/classifier.py\n",
        "import re\n",
        "from pathlib import Path\n",
        "import Levenshtein\n",
        "#use greek stemmer https://pypi.org/project/greek-stemmer/\n",
        "from greek_stemmer import GreekStemmer\n",
        "import spacy\n",
        "import spacy.symbols as POS\n",
        "\n",
        "# Load Greek Hunspell word list\n",
        "def load_word_list(path):\n",
        "    with open(path) as word_list:\n",
        "        return set([word.strip() for word in word_list])\n",
        "\n",
        "\n",
        "\n",
        "# Classifier resources\n",
        "base_dir = \"errant/errant/en\"\n",
        "# Spacy\n",
        "nlp = None\n",
        "# Greek Stemmer\n",
        "stemmer = GreekStemmer()\n",
        "# Greek Word list\n",
        "spell = load_word_list('/content/errant/el/resources/el_GR.txt')\n",
        "\n",
        "# Rare POS tags that make uninformative error categories\n",
        "rare_pos = {\"INTJ\", \"NUM\", \"SYM\", \"X\"}\n",
        "# Open class coarse Spacy POS tags \n",
        "open_pos1 = {POS.ADJ, POS.ADV, POS.NOUN, POS.VERB}\n",
        "# Open class coarse Spacy POS tags (strings)\n",
        "open_pos2 = {\"ADJ\", \"ADV\", \"NOUN\", \"VERB\"}\n",
        "# POS tags with inflectional morphology\n",
        "inflected_tags = {\"ADJ\", \"ADV\", \"AUX\", \"DET\", \"PRON\", \"PROPN\", \"NOUN\", \"VERB\"}\n",
        "# Some dep labels that map to pos tags.\n",
        "dep_map = {\"ac\": \"ADP\", \"svp\": \"ADP\",\t\"punct\": \"PUNCT\", \"CCONJ\": \"CONJ\" }\n",
        "# Accents/Vowels\n",
        "accents=['ά','έ', 'ή', 'ί', 'ό', 'ύ', 'ώ']\n",
        "#Simplified cats\n",
        "simple_cats={'CCONJ':'CONJ', 'SCONJ':'CONJ', 'ADP':'PREP' }\n",
        "\n",
        "\n",
        "# Input: An Edit object\n",
        "# Output: The same Edit object with an updated error type\n",
        "def classify(edit):  \n",
        "    # Nothing to nothing is a detected but not corrected edit\n",
        "    if not edit.o_toks and not edit.c_toks:\n",
        "        edit.type = \"UNK\"\n",
        "    # Missing\n",
        "    elif not edit.o_toks and edit.c_toks:\n",
        "        op = \"M:\"\n",
        "        cat = simplify(get_one_sided_type(edit.c_toks))\n",
        "        edit.type = op+cat   \n",
        "    # Unnecessary\n",
        "    elif edit.o_toks and not edit.c_toks:\n",
        "        op = \"U:\"\n",
        "        cat = simplify(get_one_sided_type(edit.o_toks))\n",
        "        edit.type = op+cat\n",
        "    # Replacement and special cases\n",
        "    else:\n",
        "        # Same to same is a detected but not corrected edit\n",
        "        if edit.o_str == edit.c_str:\n",
        "            edit.type = \"UNK\"\n",
        "        # Classify the edit as if the last token wasn't there\n",
        "        elif edit.o_toks[-1].lower == edit.c_toks[-1].lower and \\\n",
        "                (len(edit.o_toks) > 1 or len(edit.c_toks) > 1):\n",
        "            # Store a copy of the full orig and cor toks\n",
        "            all_o_toks = edit.o_toks[:]\n",
        "            all_c_toks = edit.c_toks[:]\n",
        "            # Truncate the instance toks for classification\n",
        "            edit.o_toks = edit.o_toks[:-1]\n",
        "            edit.c_toks = edit.c_toks[:-1]\n",
        "            # Classify the truncated edit\n",
        "            edit = classify(edit)\n",
        "            # Restore the full orig and cor toks\n",
        "            edit.o_toks = all_o_toks\n",
        "            edit.c_toks = all_c_toks\n",
        "\n",
        "        # Accent/Final Nu special cases\n",
        "        #these need to go to replacement\n",
        "        elif accent(edit.o_toks, edit.c_toks) == \"miss_acc\":\n",
        "          edit.type = \"M:ACC\"\n",
        "        elif accent(edit.o_toks, edit.c_toks) == \"unn_acc\":\n",
        "          edit.type = \"U:ACC\"\n",
        "        elif final_n(edit.o_toks, edit.c_toks) == 'unn_fn':\n",
        "          edit.type = \"U:FN\"\n",
        "        elif final_n(edit.o_toks, edit.c_toks) == 'miss_fn':\n",
        "          edit.type = \"M:FN\"\n",
        "        # Replacement\n",
        "        else:\n",
        "            op = \"R:\"\n",
        "            cat = simplify(get_two_sided_type(edit.o_toks, edit.c_toks))\n",
        "            edit.type = op+cat\n",
        "    return edit\n",
        "\n",
        "\n",
        "\n",
        "# Input: Spacy tokens\n",
        "# Output: A list of pos and dep tag strings\n",
        "def get_edit_info(toks):\n",
        "    pos = []\n",
        "    dep = []\n",
        "    for tok in toks:\n",
        "        pos.append(tok.tag_)\n",
        "        dep.append(tok.dep_)\n",
        "    return pos, dep\n",
        "\n",
        "# Input: Spacy tokens\n",
        "# Output: An error type string based on input tokens from orig or cor\n",
        "# When one side of the edit is null, we can only use the other side\n",
        "def get_one_sided_type(toks):\n",
        "    # Special cases\n",
        "    if len(toks) == 1:\n",
        "        # Subjunctive \"να\" is treated as part of a verb form\n",
        "        if toks[0].lower_ == \"να\" and toks[0].pos == POS.PART :\n",
        "            return \"VERB:FORM\"     \n",
        "        \n",
        "    # Extract pos tags and parse info from the toks\n",
        "    pos_list, dep_list = get_edit_info(toks)\n",
        "    # Auxiliary verbs e.g \"έχω, είχα\" \n",
        "        # Μέλλοντας \"θα\"\n",
        "    if toks[0].lower_ == \"θα\" and toks[0].pos == POS.PART :\n",
        "        return \"VERB:TENSE\"\n",
        "    if toks[0].pos == POS.VERB and set(dep_list).issubset({\"aux\", \"auxpass\", \"obj\", \"advmod\"}):\n",
        "        return \"VERB:TENSE\"\n",
        "    # POS-based tags. Ignores rare, uninformative categories\n",
        "    if len(set(pos_list)) == 1 and pos_list[0] not in rare_pos:\n",
        "        return pos_list[0]\n",
        "    # More POS-based tags using special dependency labels\n",
        "    if len(set(dep_list)) == 1 and dep_list[0] in dep_map.keys():\n",
        "        return dep_map[dep_list[0]]\n",
        "    # Tricky cases\n",
        "    else:\n",
        "        return \"OTHER\"\n",
        "\n",
        "\n",
        "# Input 1: Spacy orig tokens\n",
        "# Input 2: Spacy cor tokens\n",
        "# Output: An error type string based on orig AND cor\n",
        "def get_two_sided_type(o_toks, c_toks):\n",
        "    # Extract pos tags and parse info from the toks as lists\n",
        "    o_pos, o_dep = get_edit_info(o_toks)\n",
        "    c_pos, c_dep = get_edit_info(c_toks)\n",
        "\n",
        "    # Orthography; i.e. whitespace and/or case errors.\n",
        "    if only_orth_change(o_toks, c_toks):\n",
        "        return \"ORTH\"\n",
        "    # Word Order; only matches exact reordering.\n",
        "    if exact_reordering(o_toks, c_toks):\n",
        "        return \"WO\"   \n",
        "             \n",
        "\n",
        "# 2. SPELLING AND INFLECTION\n",
        "        # Only check alphabetical strings on the original side\n",
        "        # Spelling errors take precedence over POS errors; this rule is ordered\n",
        "        if o_toks[0].text.isalpha():          \n",
        "            # Check a greek dict for both orig and lower case.            \n",
        "            if o_toks[0].text not in spell and \\\n",
        "                    o_toks[0].lower_ not in spell:\n",
        "                # Check if both sides have a common lemma\n",
        "                if o_toks[0].lemma == c_toks[0].lemma:\n",
        "                    # Inflection\n",
        "                    # Spacy issue returns nonetype when does not properly assign pos\n",
        "                    if o_pos == c_pos and o_pos[0] in {\"NOUN\", \"ADJ\", \"ADV\", \"PRON\", \"VERB\"}:\n",
        "                        return o_pos[0]+\":FORM\"\n",
        "                    # Unknown morphology; i.e. we cannot be more specific.\n",
        "                    else:\n",
        "                        return \"MORPH\"\n",
        "                # Use string similarity to detect true spelling errors.\n",
        "                else:\n",
        "                  char_ratio = Levenshtein.ratio(o_toks[0].text, c_toks[0].text)\n",
        "                  # Ratio > 0.5 means both side share at least half the same chars.\n",
        "                  # WARNING: THIS IS AN APPROXIMATION.\n",
        "                  if char_ratio > 0.5:\n",
        "                      return \"SPELL\"\n",
        "                  # If ratio is <= 0.5, the error is more complex\n",
        "                  else:\n",
        "                      # If POS is the same, this takes precedence over spelling.\n",
        "                      if o_pos == c_pos and \\\n",
        "                              o_pos[0] not in rare_pos:\n",
        "                          return o_pos[0]\n",
        "                      # Tricky cases.\n",
        "                      \n",
        "                      if char_ratio > 0.9:\n",
        "                        if o_toks[0] not in conts and c_toks[0] not in conts and \\\n",
        "                        accent(o_toks,c_toks) == \"repl_acc\":\n",
        "                          return \"ACC\"\n",
        "                      else:\n",
        "                        return \"OTHER\"   \n",
        "\n",
        "        # 3. MORPHOLOGY\n",
        "        # Only ADJ, ADV, NOUN and VERB can have inflectional changes.\n",
        "        if o_toks[0].lemma == c_toks[0].lemma and \\\n",
        "                o_pos[0] in open_pos2 and \\\n",
        "                c_pos[0] in open_pos2:\n",
        "            # Same POS on both sides\n",
        "            if o_pos == c_pos:\n",
        "                # Adjective form; e.g. comparatives\n",
        "                if o_pos[0] == \"ADJ\":\n",
        "                    return \"ADJ:FORM\"\n",
        "                # Noun number\n",
        "                if o_pos[0] == \"NOUN\":\n",
        "                    return \"NOUN:FORM\"\n",
        "                # Verbs - various types\n",
        "                if o_pos[0] == \"VERB\":\n",
        "                    # NOTE: These rules are carefully ordered.\n",
        "                    # Use the dep parse to find some form errors.\n",
        "                    # Main verbs preceded by aux cannot be tense or SVA.\n",
        "                    if preceded_by_aux(o_toks, c_toks):\n",
        "                        return \"VERB:FORM\"\n",
        "                if o_pos == c_pos and o_pos[0] == \"VERB\":\n",
        "                        return \"VERB:FORM\"\n",
        "                        \n",
        "\n",
        "        # 4. GENERAL\n",
        "        # Auxiliaries with different lemmas\n",
        "        if o_dep[0].startswith(\"aux\") and c_dep[0].startswith(\"aux\"):\n",
        "            return \"VERB:TENSE\"\n",
        "        # POS-based tags. Some of these are context sensitive mispellings.\n",
        "        if o_pos == c_pos and o_pos[0] not in rare_pos:\n",
        "            return o_pos[0]\n",
        "        # Some dep labels map to POS-based tags.\n",
        "        if o_dep == c_dep and o_dep[0] in dep_map.keys():\n",
        "            return dep_map[o_dep[0]]\n",
        "        else:\n",
        "            return \"OTHER\"\n",
        "\n",
        "    # Multi-token replacements (uncommon)\n",
        "    # All auxiliaries\n",
        "    if set(o_dep+c_dep).issubset({\"aux\", \"auxpass\"}):\n",
        "        return \"VERB:TENSE\"\n",
        "    # All same POS\n",
        "    if len(set(o_pos+c_pos)) == 1:\n",
        "        # Final verbs with the same lemma are tense\n",
        "        if o_pos[0] == \"VERB\" and \\\n",
        "                o_toks[-1].lemma == c_toks[-1].lemma:\n",
        "            return \"VERB:TENSE\"\n",
        "        # POS-based tags.\n",
        "        elif o_pos[0] not in rare_pos:\n",
        "            return o_pos[0]\n",
        "    # All same special dep labels.\n",
        "    if len(set(o_dep+c_dep)) == 1 and \\\n",
        "            o_dep[0] in dep_map.keys():\n",
        "        return dep_map[o_dep[0]]\n",
        "    # Verbs with particles e.g., μην κάνεις\n",
        "    if set(o_pos+c_pos) == {\"PART\", \"VERB\"}:\n",
        "      \n",
        "        if o_toks[-1].lemma == c_toks[-1].lemma:\n",
        "            return \"VERB:FORM\"\n",
        "        # In case particle needs to go\n",
        "        else:\n",
        "            return \"VERB\"\n",
        "    # Tricky cases.\n",
        "    else:\n",
        "        return \"OTHER\"\n",
        "\n",
        "\n",
        "def only_orth_change(o_toks, c_toks):\n",
        "    o_join = \"\".join([o.lower_ for o in o_toks])\n",
        "    c_join = \"\".join([c.lower_ for c in c_toks])\n",
        "    if o_join == c_join:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "# Input 1: Spacy orig tokens\n",
        "# Input 2: Spacy cor tokens\n",
        "# Output: Boolean; the tokens are exactly the same but in a different order\n",
        "def exact_reordering(o_toks, c_toks):\n",
        "    # Sorting lets us keep duplicates.\n",
        "    o_set = sorted([o.lower_ for o in o_toks])\n",
        "    c_set = sorted([c.lower_ for c in c_toks])\n",
        "    if o_set == c_set:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "\n",
        "\n",
        "def accent(o_toks, c_toks):\n",
        "  o_toks = str(o_toks)\n",
        "  c_toks = str(c_toks)\n",
        "  o_chars =[char for char in o_toks]\n",
        "  c_chars = [char for char in c_toks]\n",
        "   \n",
        "  char_list1=[]  \n",
        "  char_list2=[]  \n",
        "\n",
        "  if set(o_chars).isdisjoint(accents) == False and set(c_chars).isdisjoint(accents) == True:\n",
        "    return \"unn_acc\"\n",
        "  elif set(o_chars).isdisjoint(accents) == True and set(c_chars).isdisjoint(accents) == False:\n",
        "   return \"miss_acc\"\n",
        "  elif set(o_chars).isdisjoint(accents) == False and set(c_chars).isdisjoint(accents) == False:\n",
        "    char1 = list(set(o_chars).intersection(accents))\n",
        "    char2 = list(set(c_chars).intersection(accents))\n",
        "    if len(char2)>1:\n",
        "      return 'miss_acc'\n",
        "    elif len(char1)>1:\n",
        "      return 'unn_acc'\n",
        "    else:\n",
        "      len(char1) == len(char2)\n",
        "      if o_chars.index(char1[0]) != c_chars.index(char2[0]): \n",
        "        return'repl_acc'\n",
        "\n",
        "    \n",
        "def final_n(o_toks, c_toks):\n",
        "  o_toks = str(o_toks)\n",
        "  c_toks = str(c_toks)\n",
        "  o_chars =[char for char in o_toks]\n",
        "  c_chars = [char for char in c_toks]\n",
        "\n",
        "  if o_chars == c_chars[:-1] and c_chars[-1]=='ν':\n",
        "    return \"miss_fn\"\n",
        "  elif o_chars[:-1] == c_chars and o_chars[-1]=='ν':\n",
        "    return \"unn_fn\"\n",
        "\n",
        "def simplify(cat):\n",
        "  if cat in simple_cats:\n",
        "    cat = simple_cats.get(cat)\n",
        "  return cat"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing errant/el/classifier.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YhwkEcgf0FaW"
      },
      "source": [
        "import errant\n",
        "import argparse\n",
        "from contextlib import ExitStack\n",
        "import errant\n",
        "\n",
        "def parallel(out, orig, cor, tok = False, merge ='rules', lev = False ):\n",
        "  print(\"Loading resources...\")\n",
        "  # Load Errant\n",
        "  annotator = errant.load(\"en\")\n",
        "  # Open output m2 file\n",
        "  out_m2 = open(out, \"w\")\n",
        "\n",
        "  print(\"Processing parallel files...\")\n",
        "  # Process an arbitrary number of files line by line simultaneously. Python 3.3+\n",
        "  # See https://tinyurl.com/y4cj4gth\n",
        "  with ExitStack() as stack:\n",
        "      in_files = [stack.enter_context(open(i)) for i in [orig]+cor]\n",
        "      # Process each line of all input files\n",
        "      for line in zip(*in_files):\n",
        "          # Get the original and all the corrected texts\n",
        "          orig = line[0].strip()\n",
        "          cors = line[1:]\n",
        "          # Skip the line if orig is empty\n",
        "          if not orig: continue\n",
        "          # Parse orig with spacy\n",
        "          orig = annotator.parse(orig, tok)\n",
        "          # Write orig to the output m2 file\n",
        "          out_m2.write(\" \".join([\"S\"]+[token.text for token in orig])+\"\\n\")\n",
        "          # Loop through the corrected texts\n",
        "          for cor_id, cor in enumerate(cors):\n",
        "              cor = cor.strip()\n",
        "              # If the texts are the same, write a noop edit\n",
        "              if orig.text.strip() == cor:\n",
        "                  out_m2.write(noop_edit(cor_id)+\"\\n\")\n",
        "              # Otherwise, do extra processing\n",
        "              else:\n",
        "                  # Parse cor with spacy\n",
        "                  cor = annotator.parse(cor, tok)\n",
        "                  # Align the texts and extract and classify the edits\n",
        "                  edits = annotator.annotate(orig, cor, lev, merge)\n",
        "                  # Loop through the edits\n",
        "                  for edit in edits:\n",
        "                      # Write the edit to the output m2 file\n",
        "                      out_m2.write(edit.to_m2(cor_id)+\"\\n\")\n",
        "          # Write a newline when we have processed all corrections for each line\n",
        "          out_m2.write(\"\\n\")\n",
        "          \n",
        "# Input: A coder id\n",
        "# Output: A noop edit; i.e. text contains no edits\n",
        "def noop_edit(id=0):\n",
        "    return \"A -1 -1|||noop|||-NONE-|||REQUIRED|||-NONE-|||\"+str(id)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bvMJ2apNVEj"
      },
      "source": [
        "parallel('out_m2','orig.txt',['corr.txt'])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}